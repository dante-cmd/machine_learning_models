{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f47b809c",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11d53962",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca5c07f",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aef4ba2",
   "metadata": {},
   "source": [
    "## Binary Classification with Tensor Flow (*Keras*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679512de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2e763263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, softmax, sigmoid, tanh \n",
    "from keras.losses import binary_crossentropy, sparse_categorical_crossentropy\n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.metrics import accuracy, Precision\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.initializers import GlorotNormal, RandomNormal, RandomUniform, VarianceScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "d52654b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "afe09e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import re\n",
    "from operator import getitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9186d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_crime = pd.read_stata(r'./data_dta/CRIME1.dta')\n",
    "# Metadata --> http://eswf.uni-koeln.de/daten/crime1.html\n",
    "\n",
    "# Generate the variable arr86, which a person is labeled equal to 0, if he or she has not arrested in 1986\n",
    "# otherwise 1.\n",
    "data_crime['arr86'] = np.where(data_crime.narr86 == 0, data_crime.narr86, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "dbe8c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr86_columns = data_crime.columns.str.match(r'(?:n(?:f|p)?)?arr86$')\n",
    "# Remove the arr86 columns\n",
    "columns_arr86_removed = data_crime.columns[~ arr86_columns]\n",
    "\n",
    "categorical = ['ptime86']\n",
    "dummies = ['black', 'hispan', 'born60']\n",
    "is_not_continuous = columns_arr86_removed.isin(dummies + categorical)\n",
    "continuous = columns_arr86_removed[~is_not_continuous]\n",
    "target =  'arr86'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34940c75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "4a1f3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData:\n",
    "    def __init__(self, dataframe, target, categorical=[], continuous=[], dummies=[], how='train') -> None:\n",
    "        \n",
    "        self.df = dataframe\n",
    "        self.target = target\n",
    "        self.categorical = categorical\n",
    "        self.continuous = continuous\n",
    "        self.dummies = dummies\n",
    "        self.how = how\n",
    "\n",
    "    def convert_to_dummies(self, dataframe, categorical):\n",
    "    \n",
    "        dummies_list = [pd.get_dummies(dataframe[cat], prefix=cat) for cat in categorical]\n",
    "        if len(dummies_list)>1:\n",
    "            return pd.concat(dummies_list, axis=1)\n",
    "        else:\n",
    "            return getitem(dummies_list, 0)\n",
    "    \n",
    "    def transform_continuous(self, dataframe, continuous):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(dataframe[continuous])\n",
    "        df_cont = self.scaler.transform(dataframe[continuous])\n",
    "        out = pd.DataFrame(df_cont, columns=[cont + '_standard' for cont in continuous])\n",
    "        return out\n",
    "\n",
    "    def transform_data_and_split(self):\n",
    "        \n",
    "        # Features\n",
    "        list_vars = []\n",
    "        if len(self.categorical) >= 1:\n",
    "            data_dummies = self.convert_to_dummies(self.df, self.categorical)\n",
    "            list_vars.append(data_dummies)\n",
    "        \n",
    "        if len(self.continuous) >= 1 :\n",
    "            data_scaled = self.transform_continuous(self.df, self.continuous)\n",
    "            list_vars.append(data_scaled)\n",
    "        if len(dummies)>=1:\n",
    "            list_vars.append(self.df[dummies])\n",
    "        \n",
    "        assert len(list_vars)>=1, \"No exists features\"\n",
    "        \n",
    "        if len(list_vars)>1:\n",
    "            X_trans = pd.concat(list_vars, axis=1).astype(np.float32).values\n",
    "        \n",
    "        else:\n",
    "            X_trans = getitem(list_vars, 0).astype(np.float32).values\n",
    "        \n",
    "\n",
    "        y = data_crime[self.target]\n",
    "        # ohe = OneHotEncoder(sparse_output=False)\n",
    "        y_trans = y.astype(np.int32).values\n",
    "        # Target\n",
    "        if self.how == 'train':\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_trans,\n",
    "                                                    y_trans,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1234)\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        else:\n",
    "            X_to_predict, y_to_predict = X_trans.copy(), y_trans.copy()\n",
    "\n",
    "            return X_to_predict, y_to_predict\n",
    "\n",
    "    def from_df_to_loader(self, X, y, batch_size):\n",
    "        X_torch = torch.from_numpy(X)\n",
    "        y_torch = torch.from_numpy(y)\n",
    "        y_torch = y_torch.type(torch.int64)\n",
    "        dataset = data_utils.TensorDataset(X_torch, y_torch)\n",
    "        loader = data_utils.DataLoader(dataset, batch_size=batch_size)\n",
    "        return loader\n",
    "\n",
    "    def to_torch(self):\n",
    "        if self.how == 'train':\n",
    "            X_train, X_test, y_train, y_test = self.transform_data_and_split()\n",
    "            train_loader = self.from_df_to_loader(X_train, y_train, 10)\n",
    "            test_loader = self.from_df_to_loader(X_test, y_test, 10)\n",
    "            \n",
    "            return train_loader, test_loader\n",
    "        else:\n",
    "            X_to_predict, y_to_predict = self.transform_data_and_split()\n",
    "            train_loader = self.from_df_to_loader(X_to_predict, y_to_predict, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "7d0cc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "getdata = GetData(data_crime,target, categorical, continuous, dummies)\n",
    "train_loader, test_loader = getdata.to_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "1abb436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 26])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "d749426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=26, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(26, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        \n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # --> shape --> (10, 2)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "c790803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "a77e14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # pred => (10, 2)\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, cumm = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{cumm:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "a5fcab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "8212ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.378230  [    0/ 2043]\n",
      "loss: 0.395267  [ 1000/ 2043]\n",
      "loss: 0.804306  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.584692 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.376952  [    0/ 2043]\n",
      "loss: 0.394415  [ 1000/ 2043]\n",
      "loss: 0.804112  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.583954 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.375664  [    0/ 2043]\n",
      "loss: 0.393549  [ 1000/ 2043]\n",
      "loss: 0.803948  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.583206 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.374364  [    0/ 2043]\n",
      "loss: 0.392719  [ 1000/ 2043]\n",
      "loss: 0.803793  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.582450 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.373056  [    0/ 2043]\n",
      "loss: 0.391875  [ 1000/ 2043]\n",
      "loss: 0.803664  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.581683 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.371734  [    0/ 2043]\n",
      "loss: 0.391028  [ 1000/ 2043]\n",
      "loss: 0.803525  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.580907 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.370404  [    0/ 2043]\n",
      "loss: 0.390182  [ 1000/ 2043]\n",
      "loss: 0.803356  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.580123 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.369063  [    0/ 2043]\n",
      "loss: 0.389341  [ 1000/ 2043]\n",
      "loss: 0.803184  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.579334 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.367718  [    0/ 2043]\n",
      "loss: 0.388512  [ 1000/ 2043]\n",
      "loss: 0.803008  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.578541 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.366364  [    0/ 2043]\n",
      "loss: 0.387678  [ 1000/ 2043]\n",
      "loss: 0.802852  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.577743 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.364987  [    0/ 2043]\n",
      "loss: 0.386833  [ 1000/ 2043]\n",
      "loss: 0.802683  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.576941 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.363616  [    0/ 2043]\n",
      "loss: 0.385992  [ 1000/ 2043]\n",
      "loss: 0.802491  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.576137 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.362234  [    0/ 2043]\n",
      "loss: 0.385176  [ 1000/ 2043]\n",
      "loss: 0.802308  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.575333 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.360831  [    0/ 2043]\n",
      "loss: 0.384352  [ 1000/ 2043]\n",
      "loss: 0.802142  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.574531 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.359434  [    0/ 2043]\n",
      "loss: 0.383548  [ 1000/ 2043]\n",
      "loss: 0.801973  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.573730 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.358036  [    0/ 2043]\n",
      "loss: 0.382753  [ 1000/ 2043]\n",
      "loss: 0.801786  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.572929 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.356655  [    0/ 2043]\n",
      "loss: 0.381979  [ 1000/ 2043]\n",
      "loss: 0.801613  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.572137 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.355257  [    0/ 2043]\n",
      "loss: 0.381212  [ 1000/ 2043]\n",
      "loss: 0.801461  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.571353 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.353864  [    0/ 2043]\n",
      "loss: 0.380459  [ 1000/ 2043]\n",
      "loss: 0.801349  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.570577 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.352483  [    0/ 2043]\n",
      "loss: 0.379726  [ 1000/ 2043]\n",
      "loss: 0.801258  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.569818 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.351094  [    0/ 2043]\n",
      "loss: 0.379002  [ 1000/ 2043]\n",
      "loss: 0.801182  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.569077 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.349748  [    0/ 2043]\n",
      "loss: 0.378311  [ 1000/ 2043]\n",
      "loss: 0.801105  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.568346 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.348441  [    0/ 2043]\n",
      "loss: 0.377633  [ 1000/ 2043]\n",
      "loss: 0.801052  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.567633 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.347103  [    0/ 2043]\n",
      "loss: 0.376967  [ 1000/ 2043]\n",
      "loss: 0.801045  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.566940 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.345749  [    0/ 2043]\n",
      "loss: 0.376300  [ 1000/ 2043]\n",
      "loss: 0.801051  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.566266 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.344411  [    0/ 2043]\n",
      "loss: 0.375647  [ 1000/ 2043]\n",
      "loss: 0.801064  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.565610 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.343082  [    0/ 2043]\n",
      "loss: 0.375029  [ 1000/ 2043]\n",
      "loss: 0.801091  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.564968 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.341781  [    0/ 2043]\n",
      "loss: 0.374447  [ 1000/ 2043]\n",
      "loss: 0.801159  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.564344 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.340516  [    0/ 2043]\n",
      "loss: 0.373928  [ 1000/ 2043]\n",
      "loss: 0.801272  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.563744 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.339270  [    0/ 2043]\n",
      "loss: 0.373406  [ 1000/ 2043]\n",
      "loss: 0.801421  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.563168 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.338020  [    0/ 2043]\n",
      "loss: 0.372900  [ 1000/ 2043]\n",
      "loss: 0.801579  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.562613 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.336799  [    0/ 2043]\n",
      "loss: 0.372427  [ 1000/ 2043]\n",
      "loss: 0.801725  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.562074 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.335632  [    0/ 2043]\n",
      "loss: 0.371966  [ 1000/ 2043]\n",
      "loss: 0.801901  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.561558 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.334475  [    0/ 2043]\n",
      "loss: 0.371527  [ 1000/ 2043]\n",
      "loss: 0.802085  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.561062 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.333344  [    0/ 2043]\n",
      "loss: 0.371100  [ 1000/ 2043]\n",
      "loss: 0.802295  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.560586 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.332251  [    0/ 2043]\n",
      "loss: 0.370676  [ 1000/ 2043]\n",
      "loss: 0.802538  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.560131 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.331182  [    0/ 2043]\n",
      "loss: 0.370279  [ 1000/ 2043]\n",
      "loss: 0.802753  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.559694 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.330153  [    0/ 2043]\n",
      "loss: 0.369911  [ 1000/ 2043]\n",
      "loss: 0.802976  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.559274 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.329145  [    0/ 2043]\n",
      "loss: 0.369541  [ 1000/ 2043]\n",
      "loss: 0.803234  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.558876 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.328146  [    0/ 2043]\n",
      "loss: 0.369187  [ 1000/ 2043]\n",
      "loss: 0.803481  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.558493 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.327191  [    0/ 2043]\n",
      "loss: 0.368842  [ 1000/ 2043]\n",
      "loss: 0.803732  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.558126 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.326284  [    0/ 2043]\n",
      "loss: 0.368522  [ 1000/ 2043]\n",
      "loss: 0.803976  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.557774 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.325402  [    0/ 2043]\n",
      "loss: 0.368219  [ 1000/ 2043]\n",
      "loss: 0.804222  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.557440 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.324532  [    0/ 2043]\n",
      "loss: 0.367926  [ 1000/ 2043]\n",
      "loss: 0.804465  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.557120 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.323697  [    0/ 2043]\n",
      "loss: 0.367649  [ 1000/ 2043]\n",
      "loss: 0.804692  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.556815 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.322881  [    0/ 2043]\n",
      "loss: 0.367369  [ 1000/ 2043]\n",
      "loss: 0.804942  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.556527 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.322078  [    0/ 2043]\n",
      "loss: 0.367087  [ 1000/ 2043]\n",
      "loss: 0.805159  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.556249 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.321334  [    0/ 2043]\n",
      "loss: 0.366825  [ 1000/ 2043]\n",
      "loss: 0.805383  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.555982 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.320607  [    0/ 2043]\n",
      "loss: 0.366563  [ 1000/ 2043]\n",
      "loss: 0.805598  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.555725 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.319898  [    0/ 2043]\n",
      "loss: 0.366326  [ 1000/ 2043]\n",
      "loss: 0.805787  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.555479 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.319229  [    0/ 2043]\n",
      "loss: 0.366091  [ 1000/ 2043]\n",
      "loss: 0.805995  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.555246 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.318571  [    0/ 2043]\n",
      "loss: 0.365854  [ 1000/ 2043]\n",
      "loss: 0.806202  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.555023 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.317940  [    0/ 2043]\n",
      "loss: 0.365633  [ 1000/ 2043]\n",
      "loss: 0.806399  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.554808 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.317338  [    0/ 2043]\n",
      "loss: 0.365417  [ 1000/ 2043]\n",
      "loss: 0.806603  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.554601 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.316750  [    0/ 2043]\n",
      "loss: 0.365190  [ 1000/ 2043]\n",
      "loss: 0.806777  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.554401 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.316205  [    0/ 2043]\n",
      "loss: 0.364977  [ 1000/ 2043]\n",
      "loss: 0.806961  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.554211 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.315662  [    0/ 2043]\n",
      "loss: 0.364762  [ 1000/ 2043]\n",
      "loss: 0.807140  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.554029 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.315121  [    0/ 2043]\n",
      "loss: 0.364556  [ 1000/ 2043]\n",
      "loss: 0.807301  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.553853 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.314609  [    0/ 2043]\n",
      "loss: 0.364345  [ 1000/ 2043]\n",
      "loss: 0.807434  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.553684 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.314127  [    0/ 2043]\n",
      "loss: 0.364132  [ 1000/ 2043]\n",
      "loss: 0.807577  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.553522 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.313658  [    0/ 2043]\n",
      "loss: 0.363925  [ 1000/ 2043]\n",
      "loss: 0.807748  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.553367 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.313177  [    0/ 2043]\n",
      "loss: 0.363728  [ 1000/ 2043]\n",
      "loss: 0.807907  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.553217 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.312708  [    0/ 2043]\n",
      "loss: 0.363533  [ 1000/ 2043]\n",
      "loss: 0.808051  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.553073 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.312259  [    0/ 2043]\n",
      "loss: 0.363352  [ 1000/ 2043]\n",
      "loss: 0.808174  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552936 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.311826  [    0/ 2043]\n",
      "loss: 0.363174  [ 1000/ 2043]\n",
      "loss: 0.808272  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552801 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.311430  [    0/ 2043]\n",
      "loss: 0.362991  [ 1000/ 2043]\n",
      "loss: 0.808364  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552671 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.311044  [    0/ 2043]\n",
      "loss: 0.362832  [ 1000/ 2043]\n",
      "loss: 0.808442  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552545 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.310662  [    0/ 2043]\n",
      "loss: 0.362665  [ 1000/ 2043]\n",
      "loss: 0.808508  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552421 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.310311  [    0/ 2043]\n",
      "loss: 0.362517  [ 1000/ 2043]\n",
      "loss: 0.808574  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552300 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.309989  [    0/ 2043]\n",
      "loss: 0.362370  [ 1000/ 2043]\n",
      "loss: 0.808658  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552180 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.309679  [    0/ 2043]\n",
      "loss: 0.362224  [ 1000/ 2043]\n",
      "loss: 0.808726  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.552066 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.309371  [    0/ 2043]\n",
      "loss: 0.362059  [ 1000/ 2043]\n",
      "loss: 0.808829  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551955 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.309066  [    0/ 2043]\n",
      "loss: 0.361912  [ 1000/ 2043]\n",
      "loss: 0.808913  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551845 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.308787  [    0/ 2043]\n",
      "loss: 0.361760  [ 1000/ 2043]\n",
      "loss: 0.809008  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551741 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.308488  [    0/ 2043]\n",
      "loss: 0.361604  [ 1000/ 2043]\n",
      "loss: 0.809119  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551635 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.308203  [    0/ 2043]\n",
      "loss: 0.361440  [ 1000/ 2043]\n",
      "loss: 0.809227  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551531 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.307937  [    0/ 2043]\n",
      "loss: 0.361295  [ 1000/ 2043]\n",
      "loss: 0.809313  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551432 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.307672  [    0/ 2043]\n",
      "loss: 0.361146  [ 1000/ 2043]\n",
      "loss: 0.809383  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551331 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.307411  [    0/ 2043]\n",
      "loss: 0.361002  [ 1000/ 2043]\n",
      "loss: 0.809478  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551238 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.307145  [    0/ 2043]\n",
      "loss: 0.360852  [ 1000/ 2043]\n",
      "loss: 0.809568  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551146 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.306890  [    0/ 2043]\n",
      "loss: 0.360704  [ 1000/ 2043]\n",
      "loss: 0.809639  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.551056 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.306642  [    0/ 2043]\n",
      "loss: 0.360566  [ 1000/ 2043]\n",
      "loss: 0.809672  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550967 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.306414  [    0/ 2043]\n",
      "loss: 0.360427  [ 1000/ 2043]\n",
      "loss: 0.809737  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550878 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.306187  [    0/ 2043]\n",
      "loss: 0.360294  [ 1000/ 2043]\n",
      "loss: 0.809785  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550790 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.305977  [    0/ 2043]\n",
      "loss: 0.360154  [ 1000/ 2043]\n",
      "loss: 0.809841  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550704 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.305756  [    0/ 2043]\n",
      "loss: 0.360000  [ 1000/ 2043]\n",
      "loss: 0.809884  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550617 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.305551  [    0/ 2043]\n",
      "loss: 0.359846  [ 1000/ 2043]\n",
      "loss: 0.809931  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550528 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.305352  [    0/ 2043]\n",
      "loss: 0.359701  [ 1000/ 2043]\n",
      "loss: 0.810012  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550441 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.305152  [    0/ 2043]\n",
      "loss: 0.359548  [ 1000/ 2043]\n",
      "loss: 0.810021  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550356 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.304967  [    0/ 2043]\n",
      "loss: 0.359406  [ 1000/ 2043]\n",
      "loss: 0.810081  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550269 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.304785  [    0/ 2043]\n",
      "loss: 0.359272  [ 1000/ 2043]\n",
      "loss: 0.810087  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550182 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.304628  [    0/ 2043]\n",
      "loss: 0.359140  [ 1000/ 2043]\n",
      "loss: 0.810118  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550095 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.304470  [    0/ 2043]\n",
      "loss: 0.359001  [ 1000/ 2043]\n",
      "loss: 0.810125  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.550012 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.304288  [    0/ 2043]\n",
      "loss: 0.358847  [ 1000/ 2043]\n",
      "loss: 0.810200  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549928 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.304087  [    0/ 2043]\n",
      "loss: 0.358702  [ 1000/ 2043]\n",
      "loss: 0.810244  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549844 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.303888  [    0/ 2043]\n",
      "loss: 0.358563  [ 1000/ 2043]\n",
      "loss: 0.810269  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549760 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.303698  [    0/ 2043]\n",
      "loss: 0.358421  [ 1000/ 2043]\n",
      "loss: 0.810284  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549677 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.303505  [    0/ 2043]\n",
      "loss: 0.358279  [ 1000/ 2043]\n",
      "loss: 0.810309  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549595 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.303312  [    0/ 2043]\n",
      "loss: 0.358140  [ 1000/ 2043]\n",
      "loss: 0.810329  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549511 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.303135  [    0/ 2043]\n",
      "loss: 0.357997  [ 1000/ 2043]\n",
      "loss: 0.810381  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549427 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.302976  [    0/ 2043]\n",
      "loss: 0.357844  [ 1000/ 2043]\n",
      "loss: 0.810358  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549344 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.302824  [    0/ 2043]\n",
      "loss: 0.357687  [ 1000/ 2043]\n",
      "loss: 0.810393  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549261 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.302658  [    0/ 2043]\n",
      "loss: 0.357539  [ 1000/ 2043]\n",
      "loss: 0.810462  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549179 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.302493  [    0/ 2043]\n",
      "loss: 0.357405  [ 1000/ 2043]\n",
      "loss: 0.810465  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549092 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.302359  [    0/ 2043]\n",
      "loss: 0.357281  [ 1000/ 2043]\n",
      "loss: 0.810520  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.549011 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.302215  [    0/ 2043]\n",
      "loss: 0.357149  [ 1000/ 2043]\n",
      "loss: 0.810607  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548929 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.302080  [    0/ 2043]\n",
      "loss: 0.357014  [ 1000/ 2043]\n",
      "loss: 0.810663  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548844 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.301946  [    0/ 2043]\n",
      "loss: 0.356888  [ 1000/ 2043]\n",
      "loss: 0.810736  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548763 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.301807  [    0/ 2043]\n",
      "loss: 0.356758  [ 1000/ 2043]\n",
      "loss: 0.810766  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548677 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.301689  [    0/ 2043]\n",
      "loss: 0.356646  [ 1000/ 2043]\n",
      "loss: 0.810835  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548592 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.301555  [    0/ 2043]\n",
      "loss: 0.356515  [ 1000/ 2043]\n",
      "loss: 0.810821  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548508 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.301424  [    0/ 2043]\n",
      "loss: 0.356391  [ 1000/ 2043]\n",
      "loss: 0.810913  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548422 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.301297  [    0/ 2043]\n",
      "loss: 0.356270  [ 1000/ 2043]\n",
      "loss: 0.810948  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548332 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.301180  [    0/ 2043]\n",
      "loss: 0.356149  [ 1000/ 2043]\n",
      "loss: 0.811018  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548246 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.301045  [    0/ 2043]\n",
      "loss: 0.356028  [ 1000/ 2043]\n",
      "loss: 0.811049  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548156 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.300926  [    0/ 2043]\n",
      "loss: 0.355915  [ 1000/ 2043]\n",
      "loss: 0.811050  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.548068 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.300814  [    0/ 2043]\n",
      "loss: 0.355793  [ 1000/ 2043]\n",
      "loss: 0.811068  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547977 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.300694  [    0/ 2043]\n",
      "loss: 0.355687  [ 1000/ 2043]\n",
      "loss: 0.811138  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547888 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.300585  [    0/ 2043]\n",
      "loss: 0.355600  [ 1000/ 2043]\n",
      "loss: 0.811096  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547800 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.300498  [    0/ 2043]\n",
      "loss: 0.355506  [ 1000/ 2043]\n",
      "loss: 0.811115  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547710 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.300412  [    0/ 2043]\n",
      "loss: 0.355406  [ 1000/ 2043]\n",
      "loss: 0.811175  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547623 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.300301  [    0/ 2043]\n",
      "loss: 0.355320  [ 1000/ 2043]\n",
      "loss: 0.811162  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547531 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.300206  [    0/ 2043]\n",
      "loss: 0.355224  [ 1000/ 2043]\n",
      "loss: 0.811274  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547442 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.300092  [    0/ 2043]\n",
      "loss: 0.355139  [ 1000/ 2043]\n",
      "loss: 0.811281  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547353 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.300000  [    0/ 2043]\n",
      "loss: 0.355040  [ 1000/ 2043]\n",
      "loss: 0.811316  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547261 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.299909  [    0/ 2043]\n",
      "loss: 0.354945  [ 1000/ 2043]\n",
      "loss: 0.811373  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547172 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.299811  [    0/ 2043]\n",
      "loss: 0.354867  [ 1000/ 2043]\n",
      "loss: 0.811478  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.547083 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.299717  [    0/ 2043]\n",
      "loss: 0.354779  [ 1000/ 2043]\n",
      "loss: 0.811490  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.546995 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.299628  [    0/ 2043]\n",
      "loss: 0.354690  [ 1000/ 2043]\n",
      "loss: 0.811645  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.546908 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.299519  [    0/ 2043]\n",
      "loss: 0.354581  [ 1000/ 2043]\n",
      "loss: 0.811755  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.546819 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.299424  [    0/ 2043]\n",
      "loss: 0.354484  [ 1000/ 2043]\n",
      "loss: 0.811762  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.546731 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.299339  [    0/ 2043]\n",
      "loss: 0.354384  [ 1000/ 2043]\n",
      "loss: 0.811840  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.546640 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.299250  [    0/ 2043]\n",
      "loss: 0.354295  [ 1000/ 2043]\n",
      "loss: 0.811944  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.546552 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.299155  [    0/ 2043]\n",
      "loss: 0.354191  [ 1000/ 2043]\n",
      "loss: 0.811996  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.546456 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.299082  [    0/ 2043]\n",
      "loss: 0.354099  [ 1000/ 2043]\n",
      "loss: 0.812028  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.546367 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.299007  [    0/ 2043]\n",
      "loss: 0.353999  [ 1000/ 2043]\n",
      "loss: 0.812000  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.546272 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.298945  [    0/ 2043]\n",
      "loss: 0.353899  [ 1000/ 2043]\n",
      "loss: 0.811994  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.546179 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.298861  [    0/ 2043]\n",
      "loss: 0.353799  [ 1000/ 2043]\n",
      "loss: 0.812054  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.546090 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.298763  [    0/ 2043]\n",
      "loss: 0.353687  [ 1000/ 2043]\n",
      "loss: 0.812000  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.546002 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.298669  [    0/ 2043]\n",
      "loss: 0.353551  [ 1000/ 2043]\n",
      "loss: 0.812050  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545910 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.298562  [    0/ 2043]\n",
      "loss: 0.353440  [ 1000/ 2043]\n",
      "loss: 0.812046  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545820 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.298450  [    0/ 2043]\n",
      "loss: 0.353319  [ 1000/ 2043]\n",
      "loss: 0.812081  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545728 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.298353  [    0/ 2043]\n",
      "loss: 0.353212  [ 1000/ 2043]\n",
      "loss: 0.812078  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545640 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.298255  [    0/ 2043]\n",
      "loss: 0.353104  [ 1000/ 2043]\n",
      "loss: 0.812023  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545546 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.298167  [    0/ 2043]\n",
      "loss: 0.352995  [ 1000/ 2043]\n",
      "loss: 0.812057  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545455 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.298045  [    0/ 2043]\n",
      "loss: 0.352879  [ 1000/ 2043]\n",
      "loss: 0.812017  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545360 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.297956  [    0/ 2043]\n",
      "loss: 0.352772  [ 1000/ 2043]\n",
      "loss: 0.811955  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545267 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.297877  [    0/ 2043]\n",
      "loss: 0.352645  [ 1000/ 2043]\n",
      "loss: 0.811877  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545172 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.297789  [    0/ 2043]\n",
      "loss: 0.352529  [ 1000/ 2043]\n",
      "loss: 0.811852  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.545075 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.297706  [    0/ 2043]\n",
      "loss: 0.352406  [ 1000/ 2043]\n",
      "loss: 0.811810  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544980 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.297602  [    0/ 2043]\n",
      "loss: 0.352277  [ 1000/ 2043]\n",
      "loss: 0.811844  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544887 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.297509  [    0/ 2043]\n",
      "loss: 0.352142  [ 1000/ 2043]\n",
      "loss: 0.811748  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544790 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.297432  [    0/ 2043]\n",
      "loss: 0.352018  [ 1000/ 2043]\n",
      "loss: 0.811782  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544693 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.297350  [    0/ 2043]\n",
      "loss: 0.351898  [ 1000/ 2043]\n",
      "loss: 0.811707  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544596 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.297258  [    0/ 2043]\n",
      "loss: 0.351772  [ 1000/ 2043]\n",
      "loss: 0.811680  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544502 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.297178  [    0/ 2043]\n",
      "loss: 0.351615  [ 1000/ 2043]\n",
      "loss: 0.811669  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544404 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.297078  [    0/ 2043]\n",
      "loss: 0.351470  [ 1000/ 2043]\n",
      "loss: 0.811642  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544306 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.296983  [    0/ 2043]\n",
      "loss: 0.351320  [ 1000/ 2043]\n",
      "loss: 0.811679  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544211 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.296887  [    0/ 2043]\n",
      "loss: 0.351177  [ 1000/ 2043]\n",
      "loss: 0.811616  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544112 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.296807  [    0/ 2043]\n",
      "loss: 0.351041  [ 1000/ 2043]\n",
      "loss: 0.811714  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.544016 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.296706  [    0/ 2043]\n",
      "loss: 0.350897  [ 1000/ 2043]\n",
      "loss: 0.811670  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.543916 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.296635  [    0/ 2043]\n",
      "loss: 0.350751  [ 1000/ 2043]\n",
      "loss: 0.811705  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.543822 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.296535  [    0/ 2043]\n",
      "loss: 0.350589  [ 1000/ 2043]\n",
      "loss: 0.811663  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.543722 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.296463  [    0/ 2043]\n",
      "loss: 0.350454  [ 1000/ 2043]\n",
      "loss: 0.811650  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.543623 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.296391  [    0/ 2043]\n",
      "loss: 0.350297  [ 1000/ 2043]\n",
      "loss: 0.811764  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.543524 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.296300  [    0/ 2043]\n",
      "loss: 0.350154  [ 1000/ 2043]\n",
      "loss: 0.811674  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.543427 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.296238  [    0/ 2043]\n",
      "loss: 0.350002  [ 1000/ 2043]\n",
      "loss: 0.811624  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.543327 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.296170  [    0/ 2043]\n",
      "loss: 0.349853  [ 1000/ 2043]\n",
      "loss: 0.811698  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.543230 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.296063  [    0/ 2043]\n",
      "loss: 0.349691  [ 1000/ 2043]\n",
      "loss: 0.811745  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.543132 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.295958  [    0/ 2043]\n",
      "loss: 0.349530  [ 1000/ 2043]\n",
      "loss: 0.811684  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.543037 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.295856  [    0/ 2043]\n",
      "loss: 0.349352  [ 1000/ 2043]\n",
      "loss: 0.811807  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.542942 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.295733  [    0/ 2043]\n",
      "loss: 0.349182  [ 1000/ 2043]\n",
      "loss: 0.811828  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.542844 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.295640  [    0/ 2043]\n",
      "loss: 0.349024  [ 1000/ 2043]\n",
      "loss: 0.811805  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542748 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.295561  [    0/ 2043]\n",
      "loss: 0.348854  [ 1000/ 2043]\n",
      "loss: 0.811756  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542647 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.295479  [    0/ 2043]\n",
      "loss: 0.348691  [ 1000/ 2043]\n",
      "loss: 0.811786  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542547 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.295394  [    0/ 2043]\n",
      "loss: 0.348534  [ 1000/ 2043]\n",
      "loss: 0.811910  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542450 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.295291  [    0/ 2043]\n",
      "loss: 0.348370  [ 1000/ 2043]\n",
      "loss: 0.811898  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542348 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.295212  [    0/ 2043]\n",
      "loss: 0.348199  [ 1000/ 2043]\n",
      "loss: 0.811885  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542250 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.295119  [    0/ 2043]\n",
      "loss: 0.348031  [ 1000/ 2043]\n",
      "loss: 0.811926  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542148 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.295026  [    0/ 2043]\n",
      "loss: 0.347852  [ 1000/ 2043]\n",
      "loss: 0.811993  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.542046 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.294915  [    0/ 2043]\n",
      "loss: 0.347682  [ 1000/ 2043]\n",
      "loss: 0.812005  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.541946 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.294845  [    0/ 2043]\n",
      "loss: 0.347509  [ 1000/ 2043]\n",
      "loss: 0.812121  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.541842 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.294757  [    0/ 2043]\n",
      "loss: 0.347337  [ 1000/ 2043]\n",
      "loss: 0.812170  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.541736 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.294687  [    0/ 2043]\n",
      "loss: 0.347174  [ 1000/ 2043]\n",
      "loss: 0.812156  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.541636 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.294623  [    0/ 2043]\n",
      "loss: 0.346997  [ 1000/ 2043]\n",
      "loss: 0.812226  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.541534 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.294545  [    0/ 2043]\n",
      "loss: 0.346817  [ 1000/ 2043]\n",
      "loss: 0.812307  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.541430 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.294458  [    0/ 2043]\n",
      "loss: 0.346637  [ 1000/ 2043]\n",
      "loss: 0.812321  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.541329 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.294376  [    0/ 2043]\n",
      "loss: 0.346449  [ 1000/ 2043]\n",
      "loss: 0.812423  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.541227 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.294280  [    0/ 2043]\n",
      "loss: 0.346262  [ 1000/ 2043]\n",
      "loss: 0.812495  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.541123 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.294202  [    0/ 2043]\n",
      "loss: 0.346083  [ 1000/ 2043]\n",
      "loss: 0.812573  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.541024 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.294121  [    0/ 2043]\n",
      "loss: 0.345882  [ 1000/ 2043]\n",
      "loss: 0.812700  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.540919 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.294039  [    0/ 2043]\n",
      "loss: 0.345695  [ 1000/ 2043]\n",
      "loss: 0.812802  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.540817 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.293952  [    0/ 2043]\n",
      "loss: 0.345498  [ 1000/ 2043]\n",
      "loss: 0.812884  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.540717 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.293860  [    0/ 2043]\n",
      "loss: 0.345306  [ 1000/ 2043]\n",
      "loss: 0.812952  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540615 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.293769  [    0/ 2043]\n",
      "loss: 0.345096  [ 1000/ 2043]\n",
      "loss: 0.813049  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540512 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.293671  [    0/ 2043]\n",
      "loss: 0.344888  [ 1000/ 2043]\n",
      "loss: 0.813188  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540412 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.293559  [    0/ 2043]\n",
      "loss: 0.344683  [ 1000/ 2043]\n",
      "loss: 0.813241  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540313 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.293453  [    0/ 2043]\n",
      "loss: 0.344476  [ 1000/ 2043]\n",
      "loss: 0.813335  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540212 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.293359  [    0/ 2043]\n",
      "loss: 0.344269  [ 1000/ 2043]\n",
      "loss: 0.813445  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540108 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.293253  [    0/ 2043]\n",
      "loss: 0.344062  [ 1000/ 2043]\n",
      "loss: 0.813503  [ 2000/ 2043]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.540009 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "6c87282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        pred = model(X)\n",
    "        print(pred.argmax(1))\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cceff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deeddb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59d9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb511ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Flatten\n",
    "# from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173f7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequential = keras.models.Sequential\n",
    "Dense, Flatten = keras.layers.Dense, keras.layers.Flatten\n",
    "activations = keras.activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066ebf6",
   "metadata": {},
   "source": [
    "## Tensor Flow for Classification\n",
    "\n",
    "Data used `Fashion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a530a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimention of features (28, 28) by each 60000 rows\n",
      "dtype uint8\n"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print('The dimention of features ({1}, {2}) by each {0} rows'.format(*X_train_full.shape))\n",
    "print(\"dtype\",X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2e611",
   "metadata": {},
   "source": [
    "* The first observation of `y_train_full` show `0`. This number represent a type of clothes.\n",
    "* The first observation of `X_train_full` is a matrix `2d` of `28x28`. In this matrix was convert the image of the clothes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3ef989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimention of features (28, 28) by each 55000 rows\n",
      "The dimention of target is (55000,) rows\n"
     ]
    }
   ],
   "source": [
    "# Create data for validation\n",
    "# The first 5000 rows is for validation, while the next 55000 rows is for training\n",
    "# We divide by 255 for work with scale the pixel intensities down to the 0-1 range\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "print('The dimention of features ({1}, {2}) by each {0} rows'.format(*X_train.shape))\n",
    "print('The dimention of target is ({},) rows'.format(*y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf4660",
   "metadata": {},
   "source": [
    "Activation function sign\n",
    "$$f(x) = -1 \\;\\text{if} \\; x<0, 1 \\;\\text{if} \\; x>=0 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04aa94c",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "* $\\alpha L$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365e7f6",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "Model:\n",
    "\n",
    "$$f(x) = \\sigma(XW + b)$$\n",
    "where $\\sigma$ = $softmax(x)$\n",
    "\n",
    "* $W$ is matrix of $n$ x $1$, where $n$ is the numbers of features.\n",
    "* $b$ is a vector of $1$ dimenstion for the computes.\n",
    "* Recall for the computation, $1$ convert in $m$, the numbers of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7758468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1ffcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c71a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0 represent:  Coat\n",
      "Unique values of y_train: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# What means each number\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(\"Category 0 represent: \",class_names[y_train[0]])\n",
    "print('Unique values of y_train: {}'.format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ada33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Model Using the Sequential API\n",
    "# We only add the layer need\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Flattens the input. Does not affect the batch size.\n",
    "    # Flatten layer whose role is simply to convert each input image into a 1D array\n",
    "    model.add(Flatten(input_shape = [28, 28]))\n",
    "    \n",
    "    # kernel_initializer --> distribution of weights when init \n",
    "    # activation function --> Relu\n",
    "    # 50 is the units (nodes or neurons) for the first hidden layer. This must positive integer\n",
    "    \n",
    "    model.add(Dense(50, activation=activations.relu, kernel_initializer = 'normal'))\n",
    "    \n",
    "    model.add(Dense(50, activation=activations.relu))\n",
    "    \n",
    "    model.add(Dense(50, activation=activations.relu))\n",
    "    \n",
    "    # Output layer. Units Output are 10. This must be equal number of classes\n",
    "    # activation softmax is because the y = [0, 1, ...2, 9]\n",
    "    model.add(Dense(10, activation=activations.softmax))\n",
    "\n",
    "    # Compile the model\n",
    "    # sparse_categorical_crossentropy -> Computes the categorical crossentropy loss where y = [0, 1, ...2, 9]\n",
    "    # sgd --> Schocastic Gradiend Descend\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer ='sgd', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02d73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "550/550 [==============================] - 2s 2ms/step - loss: 1.2755 - accuracy: 0.5846 - val_loss: 0.7581 - val_accuracy: 0.7276\n",
      "Epoch 2/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.6831 - accuracy: 0.7613 - val_loss: 0.6199 - val_accuracy: 0.7836\n",
      "Epoch 3/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.5791 - accuracy: 0.7999 - val_loss: 0.5415 - val_accuracy: 0.8122\n",
      "Epoch 4/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.5294 - accuracy: 0.8151 - val_loss: 0.4938 - val_accuracy: 0.8362\n",
      "Epoch 5/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4994 - accuracy: 0.8253 - val_loss: 0.4735 - val_accuracy: 0.8388\n",
      "Epoch 6/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4778 - accuracy: 0.8315 - val_loss: 0.4680 - val_accuracy: 0.8382\n",
      "Epoch 7/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4618 - accuracy: 0.8375 - val_loss: 0.4460 - val_accuracy: 0.8472\n",
      "Epoch 8/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4491 - accuracy: 0.8422 - val_loss: 0.4322 - val_accuracy: 0.8550\n",
      "Epoch 9/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4365 - accuracy: 0.8460 - val_loss: 0.4241 - val_accuracy: 0.8558\n",
      "Epoch 10/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4282 - accuracy: 0.8497 - val_loss: 0.4236 - val_accuracy: 0.8558\n",
      "Epoch 11/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8530 - val_loss: 0.4054 - val_accuracy: 0.8610\n",
      "Epoch 12/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4112 - accuracy: 0.8559 - val_loss: 0.4248 - val_accuracy: 0.8562\n",
      "Epoch 13/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.4060 - accuracy: 0.8575 - val_loss: 0.3926 - val_accuracy: 0.8682\n",
      "Epoch 14/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3990 - accuracy: 0.8600 - val_loss: 0.3908 - val_accuracy: 0.8642\n",
      "Epoch 15/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3935 - accuracy: 0.8615 - val_loss: 0.4604 - val_accuracy: 0.8332\n",
      "Epoch 16/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3875 - accuracy: 0.8634 - val_loss: 0.4071 - val_accuracy: 0.8508\n",
      "Epoch 17/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3823 - accuracy: 0.8656 - val_loss: 0.3774 - val_accuracy: 0.8700\n",
      "Epoch 18/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3767 - accuracy: 0.8677 - val_loss: 0.3901 - val_accuracy: 0.8652\n",
      "Epoch 19/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3719 - accuracy: 0.8691 - val_loss: 0.3819 - val_accuracy: 0.8676\n",
      "Epoch 20/20\n",
      "550/550 [==============================] - 1s 2ms/step - loss: 0.3678 - accuracy: 0.8700 - val_loss: 0.3762 - val_accuracy: 0.8720\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluating the Model\n",
    "\n",
    "model_1 = create_model()\n",
    "history = model_1.fit(\n",
    "    X_train, y_train, \n",
    "    epochs = 20, \n",
    "    batch_size = 100 , \n",
    "    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e16d39",
   "metadata": {},
   "source": [
    "The `fit()` method returns a History object containing the training parameters (`history.params`), the list of epochs it went through (`history.epoch`), and most importantly a dictionary (`history.history`) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb37d56",
   "metadata": {},
   "source": [
    "* If the data is not balanced we can pass `class_weight` \n",
    "\n",
    "    ``` python\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    model.fit(X_train, y_train, class_weight=class_weights)\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62253e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,860\n",
      "Trainable params: 44,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The summary ends with the total number of parameters, including trainable and non-trainable parameters\n",
    "# 39250 is the result of 784*50, multiply the input bu the amount of nothes in the first hidden layer\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb477e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50) (50,)\n",
      "(50, 50) (50,)\n",
      "(50, 50) (50,)\n",
      "(50, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "for layer in model_1.layers:\n",
    "    if len(layer.get_weights()) !=0:        \n",
    "        weights, biases = layer.get_weights()\n",
    "        print(weights.shape, biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa604fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_data = pd.DataFrame(history.history)\n",
    "\n",
    "columns = history_data.columns\n",
    "random_abs = np.abs(np.random.randn(len(columns), 3))\n",
    "colors = np.abs(random_abs)/random_abs.max()\n",
    "\n",
    "ys = history_data.median()\n",
    "xs = history_data.agg(lambda x:np.argsort(x)[len(x)//2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for (x, y, column, color) in zip(xs, ys, columns, colors):\n",
    "    color = tuple(color)\n",
    "    ax.plot(history_data[column], color = color, lw = 2.5)\n",
    "    \n",
    "    ax.text(x, y, column.capitalize(), fontsize = 14, color=color,  bbox=dict(boxstyle=\"round\",\n",
    "                   ec= color + (0.15,),\n",
    "                   fc= color + (0.15,),\n",
    "                   ))\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.grid(True, color = (0.1, 0.1, 0.1), alpha = 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0f9ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 98.8457 - accuracy: 0.7819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[98.84574127197266, 0.7818999886512756]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to evaluate the model\n",
    "model_1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c13558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To predict on new records\n",
    "X_new = X_test[:5].copy()\n",
    "y_proba = model_1.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e27530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ankle boot' 'Pullover' 'Trouser' 'Trouser' 'Shirt']\n"
     ]
    }
   ],
   "source": [
    "# To identify the class that each new record belong\n",
    "# get the index of the class with the greatest probability for each record\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be977b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Flattens the input. Does not affect the batch size.\n",
    "    model.add(Flatten(input_shape = [28, 28]))\n",
    "    \n",
    "    # kernel_initializer --> distribution of weights when init \n",
    "    # activation function\n",
    "    # 50 is the units output. Positive integer, dimensionality of the output space (next layer)\n",
    "    model.add(Dense(50, activation=activations.relu, kernel_initializer = 'normal'))\n",
    "    \n",
    "    model.add(Dense(50, activation=activations.relu))\n",
    "    \n",
    "    model.add(Dense(50, activation=activations.relu))\n",
    "    \n",
    "    # Output layer. Units Output are 10. This must be equal to y.shape\n",
    "    # activation sigmoid is because the y is 0-1 values\n",
    "    model.add(Dense(10, activation=activations.sigmoid))\n",
    "\n",
    "    # binary_crossentropy -> Computes the binary crossentropy loss where y = [[0, 0,.., 1], ... [0, 1,...0]]\n",
    "    # sgd --> Schocastic Gradiend Descend\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer ='sgd', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84139f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5500/5500 [==============================] - 9s 2ms/step - loss: 0.2036 - accuracy: 0.6121\n",
      "Epoch 2/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.1189 - accuracy: 0.7648\n",
      "Epoch 3/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.1012 - accuracy: 0.8038\n",
      "Epoch 4/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0925 - accuracy: 0.8207\n",
      "Epoch 5/20\n",
      "5500/5500 [==============================] - 8s 2ms/step - loss: 0.0872 - accuracy: 0.8303\n",
      "Epoch 6/20\n",
      "5500/5500 [==============================] - 9s 2ms/step - loss: 0.0834 - accuracy: 0.8370\n",
      "Epoch 7/20\n",
      "5500/5500 [==============================] - 9s 2ms/step - loss: 0.0804 - accuracy: 0.8419\n",
      "Epoch 8/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0781 - accuracy: 0.8477\n",
      "Epoch 9/20\n",
      "5500/5500 [==============================] - 9s 2ms/step - loss: 0.0760 - accuracy: 0.8514\n",
      "Epoch 10/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0744 - accuracy: 0.8557\n",
      "Epoch 11/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0729 - accuracy: 0.8578\n",
      "Epoch 12/20\n",
      "5500/5500 [==============================] - 8s 2ms/step - loss: 0.0717 - accuracy: 0.8597\n",
      "Epoch 13/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0703 - accuracy: 0.8621\n",
      "Epoch 14/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0693 - accuracy: 0.8645\n",
      "Epoch 15/20\n",
      "5500/5500 [==============================] - 7s 1ms/step - loss: 0.0682 - accuracy: 0.8668\n",
      "Epoch 16/20\n",
      "5500/5500 [==============================] - 8s 1ms/step - loss: 0.0674 - accuracy: 0.8678\n",
      "Epoch 17/20\n",
      "5500/5500 [==============================] - 7s 1ms/step - loss: 0.0665 - accuracy: 0.8705\n",
      "Epoch 18/20\n",
      "5500/5500 [==============================] - 7s 1ms/step - loss: 0.0656 - accuracy: 0.8710\n",
      "Epoch 19/20\n",
      "5500/5500 [==============================] - 7s 1ms/step - loss: 0.0649 - accuracy: 0.8723\n",
      "Epoch 20/20\n",
      "5500/5500 [==============================] - 7s 1ms/step - loss: 0.0642 - accuracy: 0.8735\n"
     ]
    }
   ],
   "source": [
    "# We transfor y = [0, 1, 0, 1, ..., 9] --> y = [[0, 0,.., 1], ... [0, 1,...0]]\n",
    "y_train_trans = tf.one_hot(y_train, 10).numpy() # 10  es number of classes\n",
    "\n",
    "model_1 = create_model()\n",
    "history = model_1.fit(X_train, y_train_trans, epochs = 20, batch_size = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6f10b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 10.6170 - accuracy: 0.8391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.617030143737793, 0.8391000032424927]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_trans = tf.one_hot(y_test, 10).numpy()\n",
    "model_1.evaluate(X_test, y_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64027d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:5]\n",
    "y_proba = model_1.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "891923f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "['Ankle boot' 'Pullover' 'Trouser' 'Trouser' 'Shirt']\n"
     ]
    }
   ],
   "source": [
    "predict_x=model_1.predict(X_new) \n",
    "y_pred=np.argmax(predict_x,axis=1)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2330cdd",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API\n",
    "\n",
    "Data for practice the regression MLP come from `fetch_california_housing()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487c5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9419cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, softmax, sigmoid, tanh \n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.initializers import GlorotNormal, RandomNormal, RandomUniform, VarianceScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5afb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file California housing data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# divide the data into [full train data] = [training data, valitation data] and [test data]\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=123)\n",
    "# divide the training data into [training data, valitation data]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full, random_state=123)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37adff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(X):\n",
    "    return X.shape[1:]\n",
    "\n",
    "def create_model_secuential(\n",
    "    input_shape,\n",
    "    activation = relu,\n",
    "    activation_init = relu, \n",
    "    units_hiden : int = 50, \n",
    "    units_out:int=1, optimizer = SGD(),\n",
    "    kernel_initializer = RandomNormal(),\n",
    "    loss ='mse'):\n",
    "    \n",
    "    seed = 124\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # kernel_initializer --> distribution of weights when init \n",
    "    # activation function\n",
    "    # 50 is the units output. Positive integer, dimensionality of the output space (next layer)\n",
    "    model.add(Dense(\n",
    "        units_hiden, \n",
    "        activation = activation_init, \n",
    "        kernel_initializer = kernel_initializer, \n",
    "        input_shape = input_shape))\n",
    "\n",
    "    model.add(Dense(units_hiden, activation=activation))\n",
    "    model.add(Dense(units_hiden, activation=activation))\n",
    "    model.add(Dense(units_out, activation=activation))\n",
    "\n",
    "    # sgd --> Schocastic Gradiend Descend\n",
    "    model.compile(loss = loss, optimizer = optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e0fde5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1161/1161 [==============================] - 3s 2ms/step - loss: 0.6162 - val_loss: 0.3810\n",
      "Epoch 2/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4015 - val_loss: 0.3435\n",
      "Epoch 3/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3634 - val_loss: 0.3316\n",
      "Epoch 4/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3505 - val_loss: 0.2951\n",
      "Epoch 5/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3531 - val_loss: 0.2986\n",
      "Epoch 6/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3398 - val_loss: 0.2981\n",
      "Epoch 7/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3292 - val_loss: 0.3200\n",
      "Epoch 8/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3228 - val_loss: 0.2883\n",
      "Epoch 9/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3272 - val_loss: 0.2886\n",
      "Epoch 10/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3106 - val_loss: 0.2961\n",
      "Epoch 11/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3103 - val_loss: 0.2659\n",
      "Epoch 12/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3036 - val_loss: 0.2688\n",
      "Epoch 13/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2951 - val_loss: 0.2571\n",
      "Epoch 14/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2929 - val_loss: 0.2715\n",
      "Epoch 15/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2883 - val_loss: 0.2554\n",
      "Epoch 16/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2879 - val_loss: 0.2663\n",
      "Epoch 17/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2852 - val_loss: 0.2560\n",
      "Epoch 18/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2796 - val_loss: 0.2552\n",
      "Epoch 19/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2918 - val_loss: 0.2687\n",
      "Epoch 20/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.2772 - val_loss: 0.2464\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.2637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26371175050735474"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train the model\n",
    "model = create_model_secuential(get_shape(X_train_scaled), optimizer = Adam())\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    epochs = 20, \n",
    "    batch_size = 10 ,\n",
    "    validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# to evaluate the model\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e06c1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.0373602],\n",
       "       [0.9118079],\n",
       "       [1.2807661],\n",
       "       [1.7926067],\n",
       "       [3.4367528]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make predictions\n",
    "X_new = X_test_scaled[:5].copy()\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d394cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.0373602],\n",
       "       [0.9118079],\n",
       "       [1.2807661],\n",
       "       [1.7926067],\n",
       "       [3.4367528]], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we save the model.\n",
    "# ww can not save a model done with `class`\n",
    "model.save('assets/neural_network/california_housing_model.h5')\n",
    "# we load the model\n",
    "model_saved = keras.models.load_model('assets/neural_network/california_housing_model.h5')\n",
    "# we evaluate the saved model\n",
    "model_saved.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a9d5a",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae30867",
   "metadata": {},
   "source": [
    "* A non-sequential neural network is a Wide & Deep neural network (by Heng-Tze Cheng et al)\n",
    "* It connects all or part of the inputs directly to the output layer.\n",
    "* This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a4d9f",
   "metadata": {},
   "source": [
    "<img src = \"https://1.bp.blogspot.com/-Dw1mB9am1l8/V3MgtOzp3uI/AAAAAAAABGs/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB/s1600/image04.png\">\n",
    "\n",
    "More information [WideDeepModel -> TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9420619e",
   "metadata": {},
   "source": [
    "The implementation is done with a `class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8027998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(keras.Model):\n",
    "\n",
    "    seed = 124\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    def __init__(self, hide_units=50, out_unit = 1, activation='relu', **kwards):\n",
    "        super().__init__(**kwards)\n",
    "        self.hide_units = hide_units\n",
    "        self.out_unit = out_unit\n",
    "        self.activation = activation\n",
    "\n",
    "    def DeepModel(self, deep_shape):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hide_units, activation=self.activation, input_shape=deep_shape))\n",
    "        model.add(Dense(self.hide_units, activation=self.activation))\n",
    "        model.add(Dense(self.hide_units, activation=self.activation))\n",
    "        model.add(Dense(self.hide_units, activation=self.activation))\n",
    "        model.add(Dense(self.out_unit, activation=self.activation))\n",
    "        return model\n",
    "\n",
    "    def CombinedModel(self, deep_shape, loss):\n",
    "        combined_model =keras.experimental.WideDeepModel(self.DeepModel(deep_shape), keras.experimental.LinearModel()) \n",
    "        combined_model.compile(optimizer=['Adam', 'Adam'], loss =loss)\n",
    "        return combined_model\n",
    "    \n",
    "    def __call__(self, linear_inputs, deep_inputs, target ,loss, epochs=20):\n",
    "        deep_shape = deep_inputs.shape[1:]\n",
    "        model_final = self.CombinedModel(deep_shape, loss)\n",
    "        model_final.fit([linear_inputs, deep_inputs], target, epochs=epochs, batch_size=10)\n",
    "        return model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "848cb35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 1.4831\n",
      "Epoch 2/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.8345\n",
      "Epoch 3/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.7258\n",
      "Epoch 4/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.6655\n",
      "Epoch 5/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.6261\n",
      "Epoch 6/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5978\n",
      "Epoch 7/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5771\n",
      "Epoch 8/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5655\n",
      "Epoch 9/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5553\n",
      "Epoch 10/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5500\n",
      "Epoch 11/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.5434\n",
      "Epoch 12/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5404\n",
      "Epoch 13/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5355\n",
      "Epoch 14/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5322\n",
      "Epoch 15/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5274\n",
      "Epoch 16/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5256\n",
      "Epoch 17/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5229\n",
      "Epoch 18/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5194\n",
      "Epoch 19/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5172\n",
      "Epoch 20/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.5151\n"
     ]
    }
   ],
   "source": [
    "widedeep = WideDeep()\n",
    "models = widedeep(\n",
    "    linear_inputs = X_train_scaled,\n",
    "    deep_inputs = X_train_scaled, \n",
    "    target = y_train, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cc8b8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.2730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27301719784736633"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d87aa1",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"assets/neural_network/wide_deep_neural_network.png\" height = 250px \\>\n",
    "    <img src = \"assets/neural_network/wide_deep_neural_network_01.png\" height = \"250px\" \\>\n",
    "</div >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb12c2b",
   "metadata": {},
   "source": [
    "* As the plot above show, we can used all the inputs both inputs for *deep* model and *wide*.\n",
    "* In the above example we used the all inputs for both, *wide* and *deep*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c92f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can not save a model done with class\n",
    "# models.save('assets/neural_network/widedeep.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19246dd",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a4d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed7fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7eae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3025d254",
   "metadata": {},
   "source": [
    "# Training Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "92616691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1f06809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, softmax, sigmoid, tanh \n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.initializers import GlorotNormal, RandomNormal, RandomUniform, VarianceScaling, HeUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d484957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.6091 - val_loss: 0.4527\n",
      "Epoch 2/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4651 - val_loss: 0.4338\n",
      "Epoch 3/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4496 - val_loss: 0.4185\n",
      "Epoch 4/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4388 - val_loss: 0.4089\n",
      "Epoch 5/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4294 - val_loss: 0.4009\n",
      "Epoch 6/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4199 - val_loss: 0.3934\n",
      "Epoch 7/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.4149 - val_loss: 0.3995\n",
      "Epoch 8/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4065 - val_loss: 0.3853\n",
      "Epoch 9/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.4026 - val_loss: 0.3852\n",
      "Epoch 10/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3988 - val_loss: 0.3830\n",
      "Epoch 11/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3955 - val_loss: 0.3659\n",
      "Epoch 12/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3910 - val_loss: 0.3605\n",
      "Epoch 13/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3889 - val_loss: 0.3657\n",
      "Epoch 14/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3857 - val_loss: 0.3611\n",
      "Epoch 15/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3837 - val_loss: 0.3622\n",
      "Epoch 16/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3829 - val_loss: 0.3582\n",
      "Epoch 17/20\n",
      "1161/1161 [==============================] - 2s 1ms/step - loss: 0.3786 - val_loss: 0.3558\n",
      "Epoch 18/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3786 - val_loss: 0.3560\n",
      "Epoch 19/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3765 - val_loss: 0.3579\n",
      "Epoch 20/20\n",
      "1161/1161 [==============================] - 2s 2ms/step - loss: 0.3743 - val_loss: 0.3488\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35350102186203003"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train the model\n",
    "model = create_model_secuential(get_shape(X_train_scaled))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    epochs = 20, \n",
    "    batch_size = 10 , \n",
    "    validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# to evaluate the model\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b71f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542be38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc9cb1e",
   "metadata": {},
   "source": [
    "# Image Segmentation\n",
    "\n",
    "Source: [TensorFlow](https://www.tensorflow.org/tutorials/images/segmentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d56fc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c3d47c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "352df52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[0.03529412, 0.02196078, 0.7866667 ],\n",
       "        [0.03529412, 0.02196078, 0.7866667 ]], dtype=float32)>,\n",
       " array([0, 1, 2]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(np.array([[9, 5.6, 200.6], [9, 5.6, 200.6]]), np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ff3a173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = tf.cast([0.1, 0.9, 5.6], tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6f7d6b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 5])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03765d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d676d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
