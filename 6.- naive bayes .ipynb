{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**: assigning a category to an input\n",
    "\n",
    "**text categorization**: assigning a label or category to an entire text or document.\n",
    "\n",
    "One of approach commom in *text categorization* is **sentiment analysis**, the positive or negative orientation that a writer expresses toward some object.\n",
    "\n",
    "The simplest version of sentiment analysis is a *binary classification* task.\n",
    "\n",
    "Others binary classification tasks are the following:\n",
    "* *Spam detection* (assigning an email to one of the two classes spam or not-spam)\n",
    "* *language id* (identifing the language of the document)\n",
    "\n",
    "The most cases in classification task are done via *supervised machine learning*, where dataset of input observations are associated with some correct output.\n",
    "\n",
    "Two of many ways of doing a classification are:\n",
    "* *Generative* classifier: Given an observation, they return the class most likely to have generated the observation. This classification is done by Naive Bayes.\n",
    "\n",
    "* *Discriminative* classifier: what features from the input are most useful to discriminate between the different possible classes. This type of classification is done by Logistic regression.\n",
    "\n",
    "Recall, discriminative classifier are more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of *supervised classification* is to take an input $D$, and a fixed set of output classes $C = c_1, ..., c_M$  and return a predicted class $c \\in C$.\n",
    "Naive Bayes is a probabilistic classifier\n",
    "\n",
    "$$\\hat{c} = \\underset{c \\in C}{\\operatorname{argmax}}\\;P(c|d) = \\underset{c \\in C}{\\operatorname{argmax}}\\;\\frac{P(d|c)P(c)}{P(d)} $$\n",
    "\n",
    "We can drop the denominator $P(d)$ since when we will be computing $\\frac{P(d|c)P(c)}{P(d)}$ for each possible class. But $P(d)$ doesn’t change for each class.\n",
    "$$\\underset{c \\in C}{\\operatorname{argmax}}\\;P(d|c)P(c) $$\n",
    "\n",
    "$P(d|c)$ is called likehood and $P(c)$ prior probability\n",
    "$d$ can be represented as a set of $n$ features $(f_1, f_2, ..., f_n)$\n",
    "\n",
    "$$\\underset{c \\in C}{\\operatorname{argmax}}\\;P(f_1, f_2, ..., f_n|c)P(c) $$\n",
    "Under  *naive Bayes assumption*  the probabilities $P(f_i|c)$ are independent given the class $c$ \n",
    "$$\\underset{c \\in C}{\\operatorname{argmax}}\\;P(f_1, f_2, ..., f_n|c)P(c)  = \\underset{c \\in C}{\\operatorname{argmax}}\\;P(f_1|c)P(f_2|c) ,..., P(f_n|c) \\; P(c)$$\n",
    "The final equation is :\n",
    "\n",
    "$$c_{NB} = \\underset{c \\in C}{\\operatorname{argmax}}\\;P(c) \\prod_{i=1}^{n} P(f_i|c)$$\n",
    "\n",
    "To apply the naive Bayes classifier to text, we need to consider word positions. $f$ is changed by $w$ meaning word.\n",
    "\n",
    "$$c_{NB} = \\underset{c \\in C}{\\operatorname{argmax}}\\;P(c) \\prod_{i \\in positions }^{n} P(w_i|c)$$\n",
    "Applying log:\n",
    "\n",
    "$$c_{NB} = \\underset{c \\in C}{\\operatorname{argmax}}\\;(P(c) + \\sum_{i \\in positions }^{n} P(w_i|c))$$\n",
    "Recall, **bag-of-words** is a set of unordered set of words (the position is ignored). \n",
    "\n",
    "And the documents passed as training will form a bag of words.\n",
    "The model will assume that a word is a feature if the word is in the documents’s bag of words.\n",
    "\n",
    "We need add-one (Laplace) smoothing to avoid zero probability.\n",
    "In the test, word that is not in bag-of-word will be ignored.\n",
    "\n",
    "Removeing the *stop words* (the, a, ...) don't improve the performance, so we work with all words in bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Extracted from [Speech and Language Processing - An Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition - Daniel Jurafsky and James H. Martin] \n",
    "Page: 63\n",
    "\n",
    "<div align = \"center\">\n",
    "  <table id=\"table_01\">\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Cat</th>\n",
    "      <th>Document</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"5\">Training</th>\n",
    "      <td>-</td>\n",
    "      <td>just plain boring</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>-</td>\n",
    "      <td>entirely predictable and lacks energy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>-</td>\n",
    "      <td>no surprises and very few laughs</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>+</td>\n",
    "      <td>very powerful</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>+</td>\n",
    "      <td>the most fun film of the summer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Test</td>\n",
    "      <td>?</td>\n",
    "      <td>predictable with no fun</td>\n",
    "    </tr>\n",
    "\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see the classes are $\\text{-}$ and $\\text{+}$\n",
    "\n",
    "* Labeling $\\text{+}$ like $1$ and the otherwise like $0$, we want to predict the phrase *predictable with no fun*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 0:'-', 1:'+'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_bayes = pd.read_csv(r'assets/naive_bayes/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naivebayes:\n",
    "    \"\"\"\n",
    "    The naive Bayes algorithm, using add-1 smoothing.\n",
    "    Extracted from [Speech and Language Processing - An Introduction to Natural Language Processing,\n",
    "    Computational Linguistics, and Speech Recognition - Daniel Jurafsky and James H. Martin]\n",
    "    Page: 62\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def token(self, sentence:str) -> list:\n",
    "        sentence_split = sentence.split(' ')\n",
    "        token_set = set(sentence_split)\n",
    "        return list(token_set)\n",
    "    \n",
    "    def fit(self, data:np.ndarray, target:np.array) -> None:\n",
    "        x = data.flatten()\n",
    "        self.classes = np.unique(target)\n",
    "        n_c, log_prior, big_doc, loglikelihood = {}, {}, {}, {}\n",
    "\n",
    "        for c in self.classes:\n",
    "            n_doc = len(target)\n",
    "            condition = target == c\n",
    "            n_c[c] = sum(condition)\n",
    "            log_prior[c] = np.log(n_c[c] / n_doc)\n",
    "            vocabulary = reduce(lambda a, b: a+b, [self.token(d) for d in x])\n",
    "            big_doc[c] = reduce(lambda a, b: a+b, [self.token(d) for d in x[condition]])\n",
    "            loglikelihood[c] = {}\n",
    "        \n",
    "            for word in vocabulary:\n",
    "                uniq_doc, count_doc = np.unique(big_doc[c], return_counts=True)\n",
    "                if np.any(word == uniq_doc):\n",
    "                    count = count_doc[uniq_doc == word][0]\n",
    "                else:\n",
    "                    count = 0\n",
    "                temp = {word:np.log((count + 1)/(len(big_doc[c]) + len(set(vocabulary))))}\n",
    "                loglikelihood[c].update(temp)\n",
    "                \n",
    "        \n",
    "        self.log_prior, self.loglikelihood, self.vocabulary = log_prior, loglikelihood, vocabulary\n",
    "\n",
    "    def predict(self, data:np.ndarray) -> np.ndarray:\n",
    "        x = data.flatten()\n",
    "        log_posterior = {}\n",
    "        results = []\n",
    "        for sentence in x:\n",
    "            for c in self.classes:\n",
    "                log_posterior[c] = self.log_prior[c]\n",
    "                word_test = self.token(sentence)\n",
    "\n",
    "                for word in word_test:\n",
    "\n",
    "                    if word in self.vocabulary:\n",
    "                        log_posterior[c] += self.loglikelihood[c][word]\n",
    "                    else:\n",
    "                        pass\n",
    "            classes_posterior = np.array(tuple(log_posterior.keys()))\n",
    "            log_posterior_final = np.array(tuple(log_posterior.values()))\n",
    "            results.append(classes_posterior[np.argmax(log_posterior_final)])\n",
    "        return np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naivebayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_bayes[['document']])\n",
    "y = np.array(data_bayes.cat)\n",
    "nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good film +\n"
     ]
    }
   ],
   "source": [
    "input_document = np.array(['predictable with no fun'])\n",
    "y_predict = nb.predict(input_document)\n",
    "for pred, doc in zip(y_predict, input_document):\n",
    "    print(doc, classes[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "* *Dealing with negation*:\n",
    "  \n",
    "  A very simple baseline that is commonly used in sentiment analysis to deal with negation is the following: during text normalization, prepend the prefix NOT to every word after a token of logical negation (n’t, not, no, never) until the next punctuation mark.\n",
    "   \n",
    "  * didn’t like this movie $\\rightarrow$ didn’t NOT_like NOT_this NOT_movie.\n",
    "\n",
    "* *Lexicons*: \n",
    "\n",
    "  In some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment we can derive the positive and negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment.\n",
    "\n",
    "  * $+$:  admirable, beautiful, confident\n",
    "  * $-$: awful, bad, bias, catastrophe\n",
    "\n",
    "  In a naive Bayes classifier we usually add a feature that is counted whenever a word from that lexicon occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "  * [1] Daniel Jurafsky and James H. Martin. Speech and Language Processing - An Introduction to Natural\n",
    "      Language Processing, Computational Linguistics, and Speech Recognition. Pages [61-64]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
