{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Methods for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Since our predictor $G(x)$ takes values in a discrete set $G$, we can always divide the input space into a collection of regions labeled according to the classification.\n",
    "\n",
    " * The boundaries of these regions can be rough or smooth, depending on the prediction\n",
    "function. For the current model  these decision boundaries are linear.\n",
    "\n",
    "* Suppose there are $K$ classes, labeled $1, 2, ..., K$, and the fitted linear model for the $k^{th}$ indicator response variable is $\\hat{f}_k(x) = \\hat{β}_{k0} + \\hat{β}_k^T x.$, That mean we compute $\\hat{\\beta}_0$ and $\\hat{\\beta}^T$ for each class\n",
    "\n",
    "* The decision boundary between class $k$ and $l$ is that set of points for which $\\hat{f}_k(x) = \\hat{f}_l(x)$, that is, the set ${x : (\\hat{β}_{k0} − \\hat{β}_{l0}) + (\\hat{β}_k − \\hat{β}_l)^T x = 0}$, an affine set or hyperplane.\n",
    "\n",
    "* Above approach is a member of *discriminant functions* $δ_k(x)$, which classify $x$ to the class with the largest value for its discriminant function.\n",
    "\n",
    "* The other approach, is that model the posterior probabilities $Pr(G = k|X = x)$. Like a Logit, $P(G = k| X =x) = \\frac{e^z}{1 + e^z}$, where $z = B_0 + B_1x$. We can compute *log-oods* $log(\\frac{Pr(G = k|X = x)}{1 -Pr(G = k|X = x)}) = z = B_0 + B_1x $. The decision boundary is defined by $\\{x|β_0 + β_T x = 0 \\}$, this since, we usually fix like a threshold $p=50\\%$, so this result in $z=0$ like a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The decision in both is linear *hyperplane*.\n",
    "\n",
    "* The methods that explicitly look for \"separating hyperplanes.\" are two. The first is the wellknown *perceptron* model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists. \n",
    "* The second method, due to Vapnik (1996), finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data.\n",
    "\n",
    "<div align = \"center\">\n",
    "  <img src = \"assets/lin_model_class/Captura_lin_for_class_01.PNG\" />  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use only $X_1, X_2, ..., X_p$ like a features, the boundary decision is the left but if we add other combnation of features like $X_1*X_2, ... ,$ or $X_1^2,X_2^2, ..$  the boundary decision is like right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $k = 1, 2, ..., K$ be the labels to classes and Suppose $f(x|G=k) = f_k(x)$ is the density function conditioned to $G=k$ and let $π_k$ be the prior probability of class $k$, with $\\sum_{k=1}^{K} π_k = 1$.\n",
    "\n",
    "Bayes Theorem  gives us:\n",
    "\n",
    "$$Pr(G = k|X = x) = \\frac{f_k(x)π_k} {f(x)} = \\frac{f_k(x)π_k} {\\sum_{l=1}^{K}f_l(x)\\pi_l} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we model each class density as multivariate Gaussian:\n",
    "\n",
    "$$f_k(x) = \\frac{1 }{(2π)^{p/2}|Σ_k|^{1/2}} e^{−1/2(x−µ_k)^T Σ_k^{-1}(x−µ_k)}$$\n",
    "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix $Σ_k = Σ_{∀k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing two classes $k$ and $ℓ$ it is sufficient to look at the log-ratio.\n",
    "\n",
    "$$log \\frac{Pr(G = k|X = x)}{Pr(G = ℓ|X = x)} = log{f_k(x)} + log{π_k} - (log{f_ℓ(x)} + log{π_ℓ})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the hyperplane $log{f_k(x)} + log{π_k} -  (log{f_ℓ(x)} + log{π_ℓ}) = 0$. This means $log{f_k(x)} + log{π_k} = log{f_ℓ(x)} + log{π_ℓ}$. \n",
    "\n",
    "As we can wee we can use one of them to compute the hyperplane. So we choose $log{f_k(x)} + log{π_k}$, this is the linear discriminant function $δ_k(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$δ_k(x) = x^TΣ^{−1}µ_k − \\frac{1}{2}µ_k^T Σ^{−1} µ_k + log π_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent description of the decision rule, with $G(x) = argmax_k{δ_k}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function $δ_k(x)$ is linear in $x$, so all the decision boundaries are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"assets/lin_model_class/Captura_lin_for_class_02.PNG\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data:\n",
    "    \n",
    "  * $\\hat{π}_k = N_k/N$, where $N_k$ is the number of class-k observations\n",
    "  * $\\hat{µ}_k = \\sum_{g_i=k}^{N} x_i/N_k$\n",
    "  * $\\hat{Σ} = \\sum_{k = 1}^{K} \\sum_{g_i = k}(x_i − \\hat{µ}_k)(x_i − \\hat{µ}_k)^T /(N − K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $δ_k(x)>δ_l(x)$ The LDA rule classifies to class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the $Σ_k$ are not assumed to be equal, then the convenient cancellations in do not occur. \n",
    "In particular the pieces quadratic in $x$ remain. We then get *quadratic discriminant functions (QDA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$δ_k(x) = −\\frac{1}{2} log|Σ_k| − \\frac{1}{2}(x − µ_k)^T Σ_{k}^{−1}(x − µ_k) + log π_k$\n",
    "\n",
    "The decision boundary between each pair of classes $k$ and $l$ is described by a quadratic equation $\\{x : δ_k(x) = δ_ℓ(x)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"assets/lin_model_class/Captura_lin_for_class_03.PNG\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The left plot shows the quadratic decision boundaries obtained using LDA\n",
    "in the five-dimensional space $X_1, X_2, X_1X_2, X_{1}^{2}, X_2^2$.\n",
    "\n",
    "* The The right plot shows the quadratic decision boundaries found by QDA in 2-dimentional space $X_1, X_2$\n",
    "* The differences are generallysmall but QDA is the preferred approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_drime = pd.read_stata(r'data_dta/CRIME1.dta')\n",
    "\n",
    "# Generate the variable arr86, where a person is labeled equal to 0, if he has not committed a crime in 1986\n",
    "# otherwise 1.\n",
    "data_drime['arr86'] = data_drime.narr86.where(data_drime.narr86==0, 1)\n",
    "\n",
    "# Selecting the features and target\n",
    "features= ['pcnv', 'avgsen', 'tottime', 'ptime86', 'qemp86']\n",
    "target = 'arr86'\n",
    "\n",
    "X = np.array(data_drime[features])\n",
    "\n",
    "# Generate polynomial and interaction features\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=X_poly.shape[1])\n",
    "X_trans = pca.fit_transform(X_poly)\n",
    "condition = pca.explained_variance_ratio_.cumsum() <= 0.997\n",
    "X_pca = X_trans[:,condition]\n",
    "\n",
    "y = np.array(data_drime[target])\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_pca, y, random_state=123, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# svd: Singular value decomposition (default). \n",
    "# Does not compute the covariance matrix, \n",
    "# therefore this solver is recommended for data with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assets.lin_model_class import utils_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7273\n",
      "Initial AUC value is 0.4515\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.97%\n",
      "> P(not committed a crime|high risk)  60.00%\n",
      "> P(committed a crime|low risk)       27.03%\n",
      "> P(committed a crime|high risk)      40.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.40%\n",
      "> P(high risk|not committed a crime)  0.60%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_metrics.assessment(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cross - Validation for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7273\n",
      "Initial AUC value is 0.4515\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.97%\n",
      "> P(not committed a crime|high risk)  60.00%\n",
      "> P(committed a crime|low risk)       27.03%\n",
      "> P(committed a crime|high risk)      40.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.40%\n",
      "> P(high risk|not committed a crime)  0.60%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_to_grid_cv = model_selection.GridSearchCV(\n",
    "\n",
    "    estimator=LinearDiscriminantAnalysis(\n",
    "        solver='lsqr'),\n",
    "    scoring= 'roc_auc',\n",
    "    cv=5,\n",
    "    param_grid={\n",
    "        'shrinkage': np.linspace(0,1, 10)},\n",
    "    verbose=False,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "fit_to_grid_cv.fit(X_train, y_train)\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', **fit_to_grid_cv.best_params_)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "utils_metrics.assessment(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_drime[features])\n",
    "\n",
    "# standrization\n",
    "scaler = StandardScaler()\n",
    "X_trans = scaler.fit_transform(X)\n",
    "\n",
    "y = np.array(data_drime[target])\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_trans, y, random_state=123, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7273\n",
      "Initial AUC value is 0.4515\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.97%\n",
      "> P(not committed a crime|high risk)  60.00%\n",
      "> P(committed a crime|low risk)       27.03%\n",
      "> P(committed a crime|high risk)      40.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.40%\n",
      "> P(high risk|not committed a crime)  0.60%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_metrics.assessment(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $B$, while at the same time ensuring that they sum to one and remain in $[0, 1]$. The model has\n",
    "the form:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(G = k|X = x) = \\frac{exp(β_{k0} + β_{k}^Tx)}{1 + \\sum_{ℓ=1}^{K-1} exp(β_{ℓ0} + β_{ℓ}^T x)} , k = 1,..., K - 1$$\n",
    "\n",
    "$$Pr(G = K|X = x) = \\frac{1}{1 + \\sum_{ℓ=1}^{K-1} exp(β_{ℓ0} + β_{ℓ}^T x)}$$\n",
    "\n",
    "* The model can be specified in terms of $K − 1$ log-odds or logit transformations\n",
    "* Although the model uses the last class as the denominator in the odds-ratios, the choice of denominator is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log \\frac{Pr(G = 1|X = x)}{Pr(G = K|X = x)} = β_{10} + β_{1}^T x$$\n",
    "\n",
    "$$log \\frac{Pr(G = 2|X = x)}{Pr(G = K|X = x)} = β_{20} + β_{2}^T x$$\n",
    "\n",
    "$$ ... $$\n",
    "\n",
    "$$log \\frac{Pr(G = k- 1|X = x)}{Pr(G = K|X = x)} = β_{(k- 1)0} + β_{(k- 1)}^T x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To emphasize the dependence on the entire parameter set $θ = \\{β_{10}, β_{1}^T , ... , β_{(K−1)0}, β_{K−1}^T \\}$, we denote the probabilities $Pr(G = k|X = x) = p_k(x; θ)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models are usually fit by maximum likelihood. The log-likelihood for $N$ observations is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta) = p_k(x; θ)  = \\prod_{i=1}^{N}p_{g_i}(x_i;\\theta) $$\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^{N}log \\;p_{gi}(x_i;\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p_k(x_i; θ) = Pr(G = k|X = x_i; θ)$. The probability of the $i^{th}$ record belong to class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $K = 2$ the model is of binary response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(G = 1|X = x) = \\frac{exp(β_{10} + β_{1}^Tx)}{1 + exp(β_{10} + β_{1}^T x)}$$\n",
    "\n",
    "$$Pr(G = 2|X = x) = \\frac{1}{1 + exp(β_{10} + β_{1}^T x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can note only we need one of these equations, since, if we find, $Pr(G=2|X=x)$, automatically we can compute $Pr(G=1|X=x)$.\n",
    "\n",
    "\n",
    "The ecuation, $Pr(G = 1|X = x) = \\frac{1}{1 + \\exp(-(β_{10} + β_{1}^T x))}$ incorpore the *sigmoid function*, $\\sigma(z)=1/( 1 + \\exp(-z)$ , with $z = β_{10} + β_{1}^T x$. This take real values and map it to range [0, 1], so it create the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize  =(9, 5))\n",
    "\n",
    "x_grid = np.linspace(-5, 5, 100)\n",
    "f = lambda x:1/(1 + np.exp(-x))\n",
    "ax.plot(x_grid, f(x_grid));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to code the two-class $g_i$ via a $0/1$ response $y_i$, where $y_i = 1$ when $g_i = 1$, and $y_i = 0$ when $g_i = 2$. Let $p_1(x; θ) = p(x; θ)$, and $p_2(x; θ) = 1 − p(x; θ)$. This means that $y_i$ ~ $Bern(p(x_i; θ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{g_i}(x_i;\\theta)= p(x_i;\\theta)^{y_i}\\;(1-p(x_i;\\theta))^{(1 - y_i)}$$\n",
    "\n",
    "Where $p_{g_i}(x_i;\\theta)$ is $PMF$ of $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example if $y_i = 1$, $g_i = 1$, and using $PMF$ we can compute $p_{1}(x_i;\\theta)= p(x_i;\\theta)$, we can see meet our suppose above given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pugging in likehood\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^{N}log \\;p(x_i;\\theta)^{y_i}\\;(1-p(x_i;\\theta))^{(1 - y_i)} $$\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^{N} y_i\\; log \\;p(x_i;\\theta) + (1 - y_i)\\;log(1-p(x_i;\\theta)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{N} y_i(β_{10} + B_1^{T}x_i) − log(1 + e^{β_{10} + B_1^{T}x_i} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the vector of inputs $x_i$ includes the constant term 1 to accommodate the intercept, $\\beta = (\\beta_{10}, \\beta_1^T )$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the log-likelihood, we set its derivatives to zero. These score equations are\n",
    "\n",
    "$$\\frac{∂l(\\theta)}{∂\\beta} = \\sum_{i=1}^{N} x_i(y_i − p(x_i; β)) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are $p+ 1$ equations nonlinear in $\\beta$\n",
    "\n",
    "To solve the score equations we use the Newton–Raphson algorithm, which requires the second-derivative or Hessian matrix\n",
    "\n",
    "$$ \\frac{∂^2l(\\beta)}{∂\\beta∂\\beta^T} = - \\sum_{i=1}^{N} x_ix_i^T p(x_i; β)(1 − p(x_i; β))$$\n",
    "\n",
    "Starting with $β_{old}$, a single Newton update is\n",
    "\n",
    "$$β_{new} = β_{old} - (\\frac{∂^2l(\\beta)}{∂\\beta∂\\beta^T})^{-1}\\frac{∂l(\\theta)}{∂\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularized Logistic Regression\n",
    "\n",
    "The $L_1$ penalty can be used to maximize a penalized version \n",
    "\n",
    "$$max_{\\beta_{10}, \\beta_{1}^T}\\{ \\sum_{i=1}^{N} y_i(β_{10} + B_1^{T}x_i) − log(1 + e^{β_{10} + B_1^{T}x_i} ) - \\lambda \\sum_{j=1}^{p}|\\beta_j| \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_drime = pd.read_stata(r'data_dta/CRIME1.dta')\n",
    "\n",
    "# Generate the variable arr86, where a person is labeled equal to 0, if he has not committed a crime in 1986\n",
    "# otherwise 1.\n",
    "data_drime['arr86'] = data_drime.narr86.where(data_drime.narr86==0, 1)\n",
    "\n",
    "# Selecting the features and target\n",
    "features= ['pcnv', 'avgsen', 'tottime', 'ptime86', 'qemp86']\n",
    "target = 'arr86'\n",
    "\n",
    "X = np.array(data_drime[features])\n",
    "y = np.array(data_drime[target])\n",
    "\n",
    "X_norm = (X - X.mean(axis=0, keepdims=True))/X.std(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "                X_norm, y, test_size=0.25)\n",
    "logit_model = LogisticRegression(solver='newton-cg')\n",
    "logit_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7243\n",
      "Initial AUC value is 0.6703\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.32%\n",
      "> P(not committed a crime|high risk)  20.00%\n",
      "> P(committed a crime|low risk)       27.68%\n",
      "> P(committed a crime|high risk)      80.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.59%\n",
      "> P(high risk|not committed a crime)  0.41%\n",
      "> P(low risk|committed a crime)       95.88%\n",
      "> P(high risk|committed a crime)      4.12%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_metrics.assessment(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, logit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(solver='liblinear'),\n",
       "             param_grid={'C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             return_train_score=True, scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "fit_to_grid_cv = model_selection.GridSearchCV(\n",
    "    # estimator=Pipeline(steps=[(\"scaler\", scaler),\\\n",
    "    #     (\"logistic\", LogisticRegression( class_weight=\"balanced\",solver='liblinear'))]),\n",
    "    estimator = LogisticRegression(solver='liblinear'),#  class_weight=\"balanced\"\n",
    "    scoring= 'roc_auc',\n",
    "    cv=5,\n",
    "    param_grid={\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'C': np.linspace(0.1,1, 10)}, \n",
    "    verbose=False,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "fit_to_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7258\n",
      "Initial AUC value is 0.4437\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.93%\n",
      "> P(not committed a crime|high risk)  66.67%\n",
      "> P(committed a crime|low risk)       27.07%\n",
      "> P(committed a crime|high risk)      33.33%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.20%\n",
      "> P(high risk|not committed a crime)  0.80%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_model = LogisticRegression(solver='liblinear', C=0.02, penalty=\"l2\")\n",
    "logit_model.fit(X_train, y_train)\n",
    "\n",
    "utils_metrics.assessment(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, logit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_drime = pd.read_stata(r'data_dta/CRIME1.dta')\n",
    "\n",
    "# Generate the variable arr86, where a person is labeled equal to 0, if he has not committed a crime in 1986\n",
    "# otherwise 1.\n",
    "data_drime['arr86'] = data_drime.narr86.where(data_drime.narr86==0, 1)\n",
    "\n",
    "# Selecting the features and target\n",
    "features= ['pcnv', 'avgsen', 'tottime', 'ptime86', 'qemp86']\n",
    "target = 'arr86'\n",
    "\n",
    "X = np.array(data_drime[features])\n",
    "y = np.array(data_drime[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `old model` is defined by normalization on features and the `new model` is defined by the generation of the polynomios of the normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OLD MODEL] Normalization \n",
    "X_norm = (X - X.mean(axis=0, keepdims=True))/X.std(axis=0, keepdims=True)\n",
    "\n",
    "# [NEW MODEL] Generating polynomial and interaction features \n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_test_data_model(X:np.ndarray):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "                X, y, test_size=0.25)\n",
    "    logit_model = LogisticRegression(solver='newton-cg')\n",
    "    logit_model.fit(X_train, y_train)\n",
    "    f1 = f1_score(y_test, logit_model.predict(X_test))\n",
    "    return f1, y_test, X_test, logit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "δ:  0.0861244019138756\n"
     ]
    }
   ],
   "source": [
    "f1_old, y_test_old, X_test_old, lm_old = f1_test_data_model(X_norm)\n",
    "f1_new, y_test_new, X_test_new, lm_new = f1_test_data_model(X_poly)\n",
    "δ_inint = f1_new - f1_old\n",
    "print(\"\\u03B4: \",δ_inint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\delta_\\text{init}$ score was computed using $(y_{test}^{new}, X_{test}^{new})$ and $(y_{test}^{old}, X_{test}^{old})$\n",
    "$$\\delta_\\text{init} = F_{1}^\\text{new} - F_{1}^\\text{old} = 0.077185$$\n",
    "\n",
    "The hypothesis are $$H_0: \\delta<=0$$ $$H_1: \\delta>0$$\n",
    "\n",
    "Given this dataset we resample it using *bootstrap technique*. This process will result in a distribution of $\\delta$. With this distribution we can calculate the $\\text{p-value}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_by_idx_model(X, y, idx, model):\n",
    "    predict = model.predict(X[idx,:])\n",
    "    return f1_score(y, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the dataset\n",
    "size = len(X_test_new)\n",
    "f1_score_boots = []\n",
    "x = 0\n",
    "while x < 1000:\n",
    "    # resampling of the dataset with replacement\n",
    "    idx = np.random.choice(range(size), size, replace=True)\n",
    "    \n",
    "    # compute the f_1 score having as an input the ressampling dataset\n",
    "    f1_score_boots_new = f1_score_by_idx_model(X_test_new, y_test_new, idx, lm_new)\n",
    "    f1_score_boots_old = f1_score_by_idx_model(X_test_old, y_test_old, idx, lm_old)\n",
    "    f1_score_boots.append({\n",
    "        'f1_score_boots_new': f1_score_boots_new,\n",
    "        'f1_score_boots_old': f1_score_boots_old\n",
    "    })\n",
    "    x += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "df_f1_score_boots = pd.DataFrame(f1_score_boots)\n",
    "# calculate the δ\n",
    "δ = df_f1_score_boots.eval('f1_score_boots_new - f1_score_boots_old')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resampling was generate under $\\delta_\\text{init}>0$, but to compute the $\\text{p-value}$ we need to keep as true the $h_0$, that is, $\\delta<=0$, but $\\delta_\\text{init}$ point out the otherwise, so we need to transform (only by localization) the $\\delta$ generade by resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26532e8a940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (9, 5))\n",
    "δ.plot(kind='hist', ax =ax , color = 'k', alpha = 0.5, label = 'δ')\n",
    "δ_shift = δ - δ_inint\n",
    "δ_shift.plot(kind='hist', ax =ax, color = 'b', alpha = 0.5, label = r'$δ_{shift}$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value:  0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"p-value: \" ,f\"{np.mean(δ_shift>δ_inint):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\text{p-value}=0\\%$  we can reject the null hypothesis and conclude new is better than old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "* The Elements of Statistical Learning Data Mining, Inference, and Prediction - Second Edition - Trevor Hastie - Robert Tibshirani - Jerome Friedman. Springer.\n",
    "\n",
    "* Data Page: https://hastie.su.domains/ElemStatLearn/\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
