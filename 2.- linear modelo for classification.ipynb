{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Methods for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Since our predictor $G(x)$ takes values in a discrete set $G$, we can always divide the input space into a collection of regions labeled according to the classification.\n",
    "\n",
    " * The boundaries of these regions can be rough or smooth, depending on the prediction\n",
    "function. For the current model  these decision boundaries are linear.\n",
    "\n",
    "* Suppose there are $K$ classes, labeled $1, 2, ..., K$, and the fitted linear model for the $kth$ indicator response variable is $\\hat{f}_k(x) = \\hat{β}_{k0} + \\hat{β}_k^T x.$\n",
    "\n",
    "* The decision boundary between class $k$ and $l$ is that set of points for which $\\hat{f}_k(x) = \\hat{f}_l(x)$, that is, the set ${x : (\\hat{β}_{k0} − \\hat{β}_{l0}) + (\\hat{β}_k − \\hat{β}_l)^T x = 0}$, an affine set or hyperplane.\n",
    "\n",
    "* Above approach is a member of *discriminant functions* $δ_k(x)$, which classify $x$ to the class with the largest value for its discriminant function.\n",
    "\n",
    "* The other approach, is that model the posterior probabilities $Pr(G = k|X = x)$. Like a Logit, $P(G = k| X =x) = \\frac{e^z}{1 + e^z}$, where $z = B_0 + B_1x$. We can compute *log-oods* $log(\\frac{Pr(G = k|X = x)}{1 -Pr(G = k|X = x)}) = z = B_0 + B_1x $. The decision boundary is defined by $\\{x|β_0 + β_T x = 0 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The decision in both is linear *hyperplane*.\n",
    "\n",
    "* The methods that explicitly look for \"separating hyperplanes.\" are two. The first is the wellknown *perceptron* model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists. \n",
    "* The second method, due to Vapnik (1996), finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data.\n",
    "\n",
    "<div align = \"center\">\n",
    "    <img src = \"assets/lin_model_class/Captura_lin_for_class_01.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use only $X_1, X_2, ..., X_p$ like a features, the boundary decision is the left but if we add other combnation of features like $X_1*X_2, ... ,$ or $X_1^2,X_2^2, ..$  the boundary decision is like right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $k = 1, 2, ..., K$ be the labels to classes and Suppose $f(x|G=k) = f_k(x)$ is the density function conditioned to $G=k$ and let $π_k$ be the prior probability of class $k$, with $\\sum_{k=1}^{K} π_k = 1$.\n",
    "\n",
    "Bayes Theorem  gives us:\n",
    "\n",
    "$$Pr(G = k|X = x) = \\frac{f_k(x)π_k} {f(x)} = \\frac{f_k(x)π_k} {\\sum_{l=1}^{K}f_l(x)\\pi_l} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we model each class density as multivariate Gaussian:\n",
    "\n",
    "$$f_k(x) = \\frac{1 }{(2π)^{p/2}|Σ_k|^{1/2}} e^{−1/2(x−µ_k)^T Σ_k^{-1}(x−µ_k)}$$\n",
    "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix $Σ_k = Σ_{∀k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing two classes $k$ and $ℓ$ it is sufficient to look at the log-ratio.\n",
    "\n",
    "$$log \\frac{Pr(G = k|X = x)}{Pr(G = ℓ|X = x)} = log{f_k(x)} + log{π_k} - (log{f_ℓ(x)} + log{π_ℓ})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the hyperplane $log{f_k(x)} + log{π_k} -  (log{f_ℓ(x)} + log{π_ℓ}) = 0$. This means $log{f_k(x)} + log{π_k} = log{f_ℓ(x)} + log{π_ℓ}$. \n",
    "\n",
    "As we can wee we can use one of them to compute the hyperplane. So we choose $log{f_k(x)} + log{π_k}$, this is the linear discriminant function $δ_k(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$δ_k(x) = x^TΣ^{−1}µ_k − \\frac{1}{2}µ_k^T Σ^{−1} µ_k + log π_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent description of the decision rule, with $G(x) = argmax_k{δ_k}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function $δ_k(x)$ is linear in $x$, so all the decision boundaries are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"./assets/lin_model_class/Captura_lin_for_class_02.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data:\n",
    "    \n",
    "  * $\\hat{π}_k = N_k/N$, where $N_k$ is the number of class-k observations\n",
    "  * $\\hat{µ}_k = \\sum_{g_i=k}^{N} x_i/N_k$\n",
    "  * $\\hat{Σ} = \\sum_{k = 1}^{K} \\sum_{g_i = k}(x_i − \\hat{µ}_k)(x_i − \\hat{µ}_k)^T /(N − K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $δ_k(x)>δ_l(x)$ The LDA rule classifies to class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the $Σ_k$ are not assumed to be equal, then the convenient cancellations in do not occur. \n",
    "In particular the pieces quadratic in $x$ remain. We then get *quadratic discriminant functions (QDA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$δ_k(x) = −\\frac{1}{2} log|Σ_k| − \\frac{1}{2}(x − µ_k)^T Σ_{k}^{−1}(x − µ_k) + log π_k$\n",
    "\n",
    "The decision boundary between each pair of classes $k$ and $l$ is described by a quadratic equation $\\{x : δ_k(x) = δ_ℓ(x)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src = \"./assets/lin_model_class/Captura_lin_for_class_03.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The left plot shows the quadratic decision boundaries obtained using LDA\n",
    "in the five-dimensional space $X_1, X_2, X_1X_2, X_{1}^{2}, X_2^2$.\n",
    "\n",
    "* The The right plot shows the quadratic decision boundaries found by QDA in 2-dimentional space $X_1, X_2$\n",
    "* The differences are generallysmall but QDA is the preferred approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_drime = pd.read_stata(r'data_dta/CRIME1.dta')\n",
    "\n",
    "# Generate the variable arr86, where a person is labeled equal to 0, if he has not committed a crime in 1986\n",
    "# otherwise 1.\n",
    "data_drime['arr86'] = data_drime.narr86.where(data_drime.narr86==0, 1)\n",
    "\n",
    "# Selecting the features and target\n",
    "features= ['pcnv', 'avgsen', 'tottime', 'ptime86', 'qemp86']\n",
    "target = 'arr86'\n",
    "\n",
    "X = np.array(data_drime[features])\n",
    "\n",
    "# Generate polynomial and interaction features\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=X_poly.shape[1])\n",
    "X_trans = pca.fit_transform(X_poly)\n",
    "condition = pca.explained_variance_ratio_.cumsum() <= 0.997\n",
    "X_pca = X_trans[:,condition]\n",
    "\n",
    "y = np.array(data_drime[target])\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_pca, y, random_state=123, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# svd: Singular value decomposition (default). \n",
    "# Does not compute the covariance matrix, \n",
    "# therefore this solver is recommended for data with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assets.lin_model_class import utils_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7273\n",
      "Initial AUC value is 0.4515\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.97%\n",
      "> P(not committed a crime|high risk)  60.00%\n",
      "> P(committed a crime|low risk)       27.03%\n",
      "> P(committed a crime|high risk)      40.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.40%\n",
      "> P(high risk|not committed a crime)  0.60%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_metrics.assessment(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cross - Validation for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7273\n",
      "Initial AUC value is 0.4515\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.97%\n",
      "> P(not committed a crime|high risk)  60.00%\n",
      "> P(committed a crime|low risk)       27.03%\n",
      "> P(committed a crime|high risk)      40.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.40%\n",
      "> P(high risk|not committed a crime)  0.60%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_to_grid_cv = model_selection.GridSearchCV(\n",
    "\n",
    "    estimator=LinearDiscriminantAnalysis(\n",
    "        solver='lsqr'),\n",
    "    scoring= 'roc_auc',\n",
    "    cv=5,\n",
    "    param_grid={\n",
    "        'shrinkage': np.linspace(0,1, 10)},\n",
    "    verbose=False,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "fit_to_grid_cv.fit(X_train, y_train)\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', **fit_to_grid_cv.best_params_)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "utils_metrics.assessment(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_drime[features])\n",
    "\n",
    "# standrization\n",
    "scaler = StandardScaler()\n",
    "X_trans = scaler.fit_transform(X)\n",
    "\n",
    "y = np.array(data_drime[target])\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_trans, y, random_state=123, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7273\n",
      "Initial AUC value is 0.4515\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.97%\n",
      "> P(not committed a crime|high risk)  60.00%\n",
      "> P(committed a crime|low risk)       27.03%\n",
      "> P(committed a crime|high risk)      40.00%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.40%\n",
      "> P(high risk|not committed a crime)  0.60%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_metrics.assessment(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, clf)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $B$, while at the same time ensuring that they sum to one and remain in $[0, 1]$. The model has\n",
    "the form:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(G = k|X = x) = \\frac{exp(β_{k0} + β_{k}^Tx)}{1 + \\sum_{ℓ=1}^{K-1} exp(β_{ℓ0} + β_{ℓ}^T x)} , k = 1,..., K - 1$$\n",
    "\n",
    "$$Pr(G = K|X = x) = \\frac{1}{1 + \\sum_{ℓ=1}^{K-1} exp(β_{ℓ0} + β_{ℓ}^T x)}$$\n",
    "\n",
    "* The model can be specified in terms of $K − 1$ log-odds or logit transformations\n",
    "* Although the model uses the last class as the denominator in the odds-ratios, the choice of denominator is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log \\frac{Pr(G = 1|X = x)}{Pr(G = K|X = x)} = β_{10} + β_{1}^T x$$\n",
    "\n",
    "$$log \\frac{Pr(G = 2|X = x)}{Pr(G = K|X = x)} = β_{20} + β_{2}^T x$$\n",
    "\n",
    "$$ ... $$\n",
    "\n",
    "$$log \\frac{Pr(G = k- 1|X = x)}{Pr(G = K|X = x)} = β_{(k- 1)0} + β_{(k- 1)}^T x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To emphasize the dependence on the entire parameter set $θ = \\{β_{10}, β_{1}^T , ... , β_{(K−1)0}, β_{K−1}^T \\}$, we denote the probabilities $Pr(G = k|X = x) = p_k(x; θ)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $K = 2$ the model is of binary response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models are usually fit by maximum likelihood. The log-likelihood for $N$ observations is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta) = p_k(x; θ)  = \\prod_{i=1}^{N}p_{g_i}(x_i;\\theta) $$\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^{N}log \\;p_{gi}(x_i;\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p_k(x_i; θ) = Pr(G = k|X = x_i; θ)$. The probability of the $i^{th}$ record belong to class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of binary response, the two-class case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(G = 1|X = x) = \\frac{exp(β_{10} + β_{1}^Tx)}{1 + exp(β_{10} + β_{1}^T x)}$$\n",
    "\n",
    "$$Pr(G = 2|X = x) = \\frac{1}{1 + exp(β_{10} + β_{1}^T x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to code the two-class $g_i$ via a $0/1$ response $y_i$, where $y_i = 1$ when $g_i = 1$, and $y_i = 0$ when $g_i = 2$. Let $p_1(x; θ) = p(x; θ)$, and $p_2(x; θ) = 1 − p(x; θ)$. This means that $y_i$ ~ $Bern(p(x_i; θ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{g_i}(x_i;\\theta)= p(x_i;\\theta)^{y_i}\\;(1-p(x_i;\\theta))^{(1 - y_i)}$$\n",
    "\n",
    "Where $p_{g_i}(x_i;\\theta)$ is $PMF$ of $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example if $y_i = 1$, $g_i = 1$, and using $PMF$ we can compute $p_{1}(x_i;\\theta)= p(x_i;\\theta)$, we can see meet our suppose above given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pugging in likehood\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^{N}log \\;p(x_i;\\theta)^{y_i}\\;(1-p(x_i;\\theta))^{(1 - y_i)} $$\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^{N} y_i\\; log \\;p(x_i;\\theta) + (1 - y_i)\\;log(1-p(x_i;\\theta)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{N} y_i(β_{10} + B_1^{T}x_i) − log(1 + e^{β_{10} + B_1^{T}x_i} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the vector of inputs $x_i$ includes the constant term 1 to accommodate the intercept, $\\beta = (\\beta_{10}, \\beta_1^T )$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the log-likelihood, we set its derivatives to zero. These score equations are\n",
    "\n",
    "$$\\frac{∂l(\\theta)}{∂\\beta} = \\sum_{i=1}^{N} x_i(y_i − p(x_i; β)) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are $p+ 1$ equations nonlinear in $\\beta$\n",
    "\n",
    "To solve the score equations we use the Newton–Raphson algorithm, which requires the second-derivative or Hessian matrix\n",
    "\n",
    "$$ \\frac{∂^2l(\\beta)}{∂\\beta∂\\beta^T} = - \\sum_{i=1}^{N} x_ix_i^T p(x_i; β)(1 − p(x_i; β))$$\n",
    "\n",
    "Starting with $β_{old}$, a single Newton update is\n",
    "\n",
    "$$β_{new} = β_{old} - (\\frac{∂^2l(\\beta)}{∂\\beta∂\\beta^T})^{-1}\\frac{∂l(\\theta)}{∂\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularized Logistic Regression\n",
    "\n",
    "The $L_1$ penalty can be used to maximize a penalized version \n",
    "\n",
    "$$max_{\\beta_{10}, \\beta_{1}^T}\\{ \\sum_{i=1}^{N} y_i(β_{10} + B_1^{T}x_i) − log(1 + e^{β_{10} + B_1^{T}x_i} ) - \\lambda \\sum_{j=1}^{p}|\\beta_j| \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_drime = pd.read_stata(r'data_dta/CRIME1.dta')\n",
    "\n",
    "# Generate the variable arr86, where a person is labeled equal to 0, if he has not committed a crime in 1986\n",
    "# otherwise 1.\n",
    "data_drime['arr86'] = data_drime.narr86.where(data_drime.narr86==0, 1)\n",
    "\n",
    "# Selecting the features and target\n",
    "features= ['pcnv', 'avgsen', 'tottime', 'ptime86', 'qemp86']\n",
    "target = 'arr86'\n",
    "\n",
    "X = np.array(data_drime[features])\n",
    "\n",
    "# Generate polynomial and interaction features\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=X_poly.shape[1])\n",
    "X_trans = pca.fit_transform(X_poly)\n",
    "condition = pca.explained_variance_ratio_.cumsum() <= 0.997\n",
    "X_pca = X_trans[:,condition]\n",
    "\n",
    "y = np.array(data_drime[target])\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_pca, y, random_state=123, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "logit_model = LogisticRegression()\n",
    "logit_model.fit(X_train, y_train)\n",
    "\n",
    "# svd: Singular value decomposition (default). \n",
    "# Does not compute the covariance matrix, \n",
    "# therefore this solver is recommended for data with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7258\n",
      "Initial AUC value is 0.4437\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.93%\n",
      "> P(not committed a crime|high risk)  66.67%\n",
      "> P(committed a crime|low risk)       27.07%\n",
      "> P(committed a crime|high risk)      33.33%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.20%\n",
      "> P(high risk|not committed a crime)  0.80%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_metrics.assessment(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, logit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation for Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(solver='liblinear'),\n",
       "             param_grid={'C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             return_train_score=True, scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "fit_to_grid_cv = model_selection.GridSearchCV(\n",
    "    # estimator=Pipeline(steps=[(\"scaler\", scaler),\\\n",
    "    #     (\"logistic\", LogisticRegression( class_weight=\"balanced\",solver='liblinear'))]),\n",
    "    estimator = LogisticRegression(solver='liblinear'),#  class_weight=\"balanced\"\n",
    "    scoring= 'roc_auc',\n",
    "    cv=5,\n",
    "    param_grid={\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'C': np.linspace(0.1,1, 10)}, \n",
    "    verbose=False,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "fit_to_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.7258\n",
      "Initial AUC value is 0.4437\n",
      "\n",
      "Probability Conditioned on Predicts\n",
      "> P(not committed a crime|low risk)   72.93%\n",
      "> P(not committed a crime|high risk)  66.67%\n",
      "> P(committed a crime|low risk)       27.07%\n",
      "> P(committed a crime|high risk)      33.33%\n",
      "\n",
      "Probability Conditioned on Outcomes\n",
      "> P(low risk|not committed a crime)   99.20%\n",
      "> P(high risk|not committed a crime)  0.80%\n",
      "> P(low risk|committed a crime)       98.92%\n",
      "> P(high risk|committed a crime)      1.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_model = LogisticRegression(solver='liblinear', C=0.02, penalty=\"l2\")\n",
    "logit_model.fit(X_train, y_train)\n",
    "\n",
    "utils_metrics.assessment(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_pred(y_test, X_test, logit_model)\n",
    "utils_metrics.prob_cond_out(y_test, X_test, logit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "* The Elements of Statistical Learning Data Mining, Inference, and Prediction - Second Edition - Trevor Hastie - Robert Tibshirani - Jerome Friedman. Springer.\n",
    "\n",
    "* Data Page: https://hastie.su.domains/ElemStatLearn/\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
